% Proof of Theorem 6.2 (section C.2, page 36 in YangXieWang)
% Questions to ponder:
% Should we have two 'delta's?


\begin{lem}[Rotation invariance]
  Let $(X_i)_{i=1}^n$ be independent, centered and sub-gaussian. Then
  $\sum_{i=1}^n X_i$ is centered and sub-gaussian with
  \begin{equation*} %Todo find C
    \norm{\sum_{i=1}^n X_i}^2_{\psi_2} \leq C \sum_{i=1}^n \norm{X_i}_{\psi_2}^2
  \end{equation*}
  \label{lem:vershynin5_9}
\end{lem}
\begin{proof}
  See [Vershynin 2010, p. 12]. %todo reference
\end{proof}

\begin{defn}[Sub-exponential norm]
  For a random variable define
  \begin{equation*}
    \norm{X}_{\psi_1} = \sup_{p\geq 1} p^{-1} \norm{X}_p
  \end{equation*}
  called the sub-exponential norm, said to 'exist' if finite.
  In that case $X$ is said to be 'sub-exponential'.
\end{defn}

\begin{lem}[Sub-gaussian squared is sub-exponential]
  A random variable $X$ is sub-gaussian if and only if $X^2$ is sub-exponential
  and
  \begin{equation*}
    \norm{X}^2_{\psi_2} \leq \norm{X^2}_{\psi_1} \leq 2 \norm{X}_{\psi_2}^2
  \end{equation*}
  \label{lem:vershynin5_14}
\end{lem}
\begin{proof}
  See [Vershynin 2010, p. 14]
\end{proof}

\begin{prop}
  \label{lem:norm12ineq}
  Let $v$ be a random vector in $\R^n$ then
  \[ \E \norm{v}_1 \leq \sqrt{n} \sqrt{ \E \norm{v}_2^2 } \]
\end{prop}

\begin{proof}
  Denote $v$'s coordinates $v=(v_1, \dots, v_n)$.
  Cauchy-Schwarz applied to some vector $w$ and $(1, \dots, 1)$ yields
  \[ \norm{w}_1 \leq \sqrt{n} \norm{w}_2 \]
  Now let $w = (\E v_1, \dots, \E v_n)$.
  Then by linearity of expectation and Jensens inequality
  \[ \E \norm{v} = \norm{w} \leq \sqrt{n} \sqrt{\sum_{i=1}^n (\E v_i)^2}
  \leq \sqrt{n} \sqrt{\E \sum_{i=1}^n v_i^2} = \sqrt{n} \sqrt{\E \norm{v}_2^2} \]
\end{proof}

\begin{thm}[Bernstein's inequality]
  Suppose $U_1, \dots, U_n$ are independent with $\E U_i = 0$, $\abs{U_i} \leq M$
  for all $i \in [n]$. Then for all $t>0$
  \[ \Prob\left(\abs{\sum_{i=1}^n U_i} \geq t\right) \leq
  \exp\left( \frac{-t^2}{2/3 M t + 2 \sigma^2} \right) \]
  where $\sigma^2 = \sum_{i=1}^n V(U_i)$.
  \label{thm:Bernstein}
\end{thm}

\begin{proof}[Proof of \cref{thm:oneStep}]
  First some introductory fixing of notation and variables.
  Fix a minimal $\delta$-covering of $\Cal{F}$
  with centers $f_1, \dots, f_{N_\delta}$.
  Define
  \[ \wt{Q} := \argmin_{f \in \Cal{F}} \norm{f - TQ}_{\nu}^2 \]
  \[ k^* := \argmin_{k \in [N_\delta]} \norm{f_k - \wh{Q}}_\infty \]
  and $ X_i := (S_i, A_i) $.
  Notice that $\wt{Q}$ differs from $\wh{Q}$ in that
  $\wt{Q}$ approximates $TQ$ w.r.t. $\norm{\cdot}^2_\nu$ while
  $\wh{Q}$ approximates $Y = (Y_1, \dots, Y_n)$ in mean squared error over
  $X = (X_1, \dots, X_n)$.
  %We define a shorthand for the average by
  %$\ol{f(X)} := 1/n \cdot \sum_{i=1}^n f(X_i)$.
  %In general we shall loosely write \emph{vectored} equations like
  %$ Y = R + \gamma \max_{a \in \Cal{A}} Q(S',a) $ 
  %(this is then meant to be equivalent to the definition of $Y$).
  We shall be loose about applying functions to vectors
  (of random variables)
  in the sense that they are applied entry-wise.
  We use $\norm{\cdot}_p$ to denote the (finite dimensional) $p$-norm
  ($p$ ommitted when $p=2$).
  When talking about $p$-norms on the random variables we always specify
  the distribution (e.g. $\norm{\cdot}_\nu$).
  When the sample (e.g. $X$) is clear from context we omit it writing
  $\norm{f} = \norm{f(X)}$.
  
  \textbf{Step 1}
  By definion (of $\wh{Q}$) for all $f \in \Cal{F}$ we have
  $\norm{\wh{Q}(X) - Y}^2 \leq \norm{f(X) - Y}^2$, leading to
  \begin{align}
    & \norm{Y}^2 + \norm{\wh{Q}}^2 - 2 Y\cdot \wh{Q}
    \leq \norm{Y}^2 + \norm{f}^2 - 2 Y\cdot f
    \\ \iff & \norm{\wh{Q}}^2 + \norm{TQ}^2 - 2 \wh{Q}\cdot TQ 
    \leq \norm{f}^2 + \norm{TQ}^2 - 2 f\cdot TQ + 2 Y\cdot \wh{Q}
    - 2 Y \cdot f - 2 \wh{Q}\cdot TQ + 2f\cdot TQ
    \\ \iff & \norm{\wh{Q} - TQ}^2
    \leq \norm{f - TQ}^2 + 2(Y - TQ)\cdot(\wh{Q} - f) 
    \\ \iff & \norm{\wh{Q} - TQ}^2
    \leq \norm{f - TQ}^2 + 2 \xi \cdot (\wh{Q} - f)
    \label{eq:c2first}
  \end{align}
  Where $ \xi_i := Y_i - TQ(X_i) $ and $\xi := (\xi_1, \dots, \xi_n)$.
  Let $\Sigma = (X_1, \dots, X_n)^{-1}(\bb{B}_n) \in \Cal{H}$
  be the $\sigma$-algebra generated by the samples.
  Now we proof a minor lemma
  \begin{prop}
    $\E(\xi_i \mid \Sigma) = 0$ and thus
    $\E(\xi_i g(X_i)) = 0$ for any function $g:\R\to \R$.
    \label{lem:YTQ}
  \end{prop}
  \begin{proof}
    Recall that $X_i = (S_i, A_i)$,
    \begin{align*}
      Y_i = R_i + \gamma \max_{a \in \Cal{A}} Q(S_{i+1}, a)
    \end{align*}
    where $S_{i+1} \sim P(X_i)$, $R_i \sim R(X_i)$ and
    \begin{align*}
      TQ(X_i) = \E_\Sigma R'_i
      + \gamma \E_\Sigma Q(S', \argmax_{a \in \Cal{A}} Q(S', a))
    \end{align*}
    where $S' \sim P(X_i)$, $R'_i \sim R(X_i)$.
    Since $S'$ and $S_{i+1}$ are i.i.d.
    \begin{align*}
      \E_\Sigma \xi_i & = \E_\Sigma \left( Y_i - TQ(X_i) \right)
      \\ & = \E_\Sigma R_i - \E_\Sigma R'_i
      + \gamma \left( \E_\Sigma \left( \max_{a \in\Cal{A}} Q(S_{i+1}, a) \right)
      - \E_\Sigma \argmax_{a \in \Cal{A}} \left(  Q(S', a) \right) \right)
      \\ & = 0 
    \end{align*}
    Therefore $\E(\xi_i \mid \Sigma) = 0$.
  \end{proof}
  By this lemma we can deduce
  \begin{equation}
    \E \left( \xi \cdot (\wh{Q} - f) \right)
    = \E \left( \xi \cdot (\wh{Q} - TQ) \right)
  \end{equation}
  To bound this we insert $f_{k^*}$ by the triangle inequality
  \begin{equation}
    \abs{\E \left( \xi \cdot (\wh{Q} - TQ) \right) }
    \leq \abs{\E \left( \xi \cdot (\wh{Q} - f_{k^*}) \right) } 
    + \abs{\E \left( \xi \cdot (f_{k^*} - TQ) \right) }
    \label{eq:c2triangle1}
  \end{equation}
  We now bound these two terms. The first by Cauchy-Schwarz
  \begin{equation}
    \abs{\E \xi \cdot (\wh{Q} - f_{k^*})}
    \leq \E \left( \norm{\xi} \norm{\wh{Q} - f_{k^*}} \right)
    \leq \E (\norm{\xi}) \sqrt{n} \delta
    \leq 2 n V_{\max} \delta
    \label{eq:c2firstcs}
  \end{equation}
  where we have used that $\norm{\wh{Q} - f_{k^*}}_\infty \leq \delta$ so
  \begin{equation}
    \norm{\wh{Q} - f_{k^*}}^2
    = \sum_{i=1}^n (\wh{Q}(X_i) - f_{k^*}(X_i))^2
    \leq \sum_{i=1}^n \delta^2
    = n \delta^2
  \end{equation}
  and that $\abs{Y_i}, TQ(X_i) \leq V_{\max}$ so
  \begin{equation}
    \norm{\xi}^2 = \sum_{i=1}^n (Y_i - TQ(X_i))^2 
    \leq \sum_{i=1}^n (2 V_{\max})^2
    = 4 V_{\max}^2 n
  \end{equation}
  To bound the second term in \cref{eq:c2triangle1} define
  \begin{equation}
  Z_j := \xi \cdot (f_j - TQ) \norm{f_j - TQ}^{-1}
  \end{equation}
  Note that since $\xi_i$ are centered $Z_j$.
  For a sub-$\sigma$-algebra $\Sigma$ define the \emph{sub-gaussian} norm by
  \begin{defn}[Sub-gaussian norm]
    \[ \norm{W}_{\psi_2,\Sigma} \defeq
    \sup_{p\geq 1} p^{-1/2} \left( \E_{\Sigma} \abs{W}^p \right)^{1/p} \]
  \end{defn}
  Because of \cref{lem:YTQ} $\xi_i (f_j(X_i) - TQ(X_i))$ is centered for any
  $i \in [n]$ and
  \begin{align}
    \norm{\xi_i(f_j(X_i) - TQ(X_i))}_{\psi_2, \Sigma}
    \leq &\; 2 V_{\max} \abs{f_j(X_i) - TQ(X_i)}
  \end{align}
  Therefore by \cref{lem:vershynin5_9}
  \begin{align}
    \norm{Z_j}_{\psi_2, \Sigma}^2
    \leq & \norm{f_j - TQ}^{-2}
    \norm{\sum_{i=1}^n \xi_i (f_j(X_i) - TQ(X_i))}_{\psi_2, \Sigma}^2
    \\ \leq &\; \norm{f_j - TQ}^{-2}
    C_1 \sum_{i=1}^n \norm{\xi_i(f_j(X_i) - TQ(X_i))}_{\psi_2, \Sigma}^2
    \\ \leq &\; \norm{f_j - TQ}^{-2}
    C_1 \sum_{i=1}^n 4 V_{\max} \abs{f_j(X_i) - TQ(X_i)}^2
    \\ = &\; 4 V_{\max}^2 C_1
  \end{align}
  Observe that $\norm{X}_p \leq \sqrt{p} \; \sup_{p\geq 1} \norm{X}_{\psi_2}$.
  Therefore by \cref{lem:vershynin5_14} we can say
  \begin{align}
    \E Z_{k^*}^2 = &\; \E \left( \E_\Sigma Z_{k^*}^2 \right)
    \\ \leq &\; \E \left( \max_{j \in [N_\delta]} \E_\Sigma Z_j^2 \right)
    \\ \leq &\; \E \left( \sqrt{2} \norm{Z_j^2}_{\psi_2, \Sigma} \right)
    \\ \leq &\; 2 \sqrt{2} \E \norm{Z_j}_{\psi_2, \Sigma}^2
    \\ \leq &\; 8 \sqrt{2} V_{\max}^2 C_1
  \end{align}
  So now we can bound
  \begin{align}
    \E \left( \xi \cdot (f_{k^*} - TQ) \right)
    &= \E \left( \norm{f_{k^*} - TQ} \abs{Z_{k^*}} \right)
    \label{eq:long1_1}
    \\ &\leq 
    \E \left( \left(\norm{\wh{Q} - TQ} + \norm{\wh{Q} - f_{k^*}} \right)
    \abs{Z_{k^*}} \right) 
    \label{eq:long1_2}
    \\ &\leq 
    \E \left( \left(\norm{\wh{Q} - TQ} + n \delta \right)
    \abs{Z_{k^*}} \right) 
    \label{eq:long1_3}
    \\ &\leq 
    \left( \E \left(\norm{\wh{Q} - TQ} + n \delta \right)^2 \right)^{1/2}
    \left( \E Z_{k^*}^2 \right)^{1/2} 
    \label{eq:long1_4}
    \\ &\leq 
    \E \left(\norm{\wh{Q} - TQ} + n \delta \right) 
    \left( \E Z_{k^*}^2 \right)^{1/2} 
    \label{eq:long1_5}
    \\ &\leq 
    \left(\sqrt{ \E \norm{\wh{Q} -TQ}_2^2 } + n \delta \right) 
    \left( \E Z_{k^*}^2 \right)^{1/2} 
    \label{eq:long1_6}
    \\ &\leq \left(\sqrt{\E \norm{\wh{Q} -TQ}_2^2} + n \delta \right)
    2^{7/4} V_{\max} \sqrt{C_1}
    \label{eq:long1_8}
  \end{align}
  %TODO correct the rest of the proof with right constants..
  Where \cref{eq:long1_1} to \cref{eq:long1_2} is by the triangle inequality,
  \cref{eq:long1_5} to \cref{eq:long1_6} is \cref{lem:norm12ineq}
  and \cref{eq:long1_6} to \cref{eq:long1_8} is due to that
  Combining \cref{eq:c2first}, \cref{eq:c2triangle1},
  \cref{eq:c2firstcs} and \cref{eq:long1_8}
  \begin{align}
    \E \norm{\wh{Q} - TQ}^2 & \leq \E \norm{f - TQ}^2 + 4 n V_{\max} \delta
    + \left( \sqrt{\E \norm{\wh{Q} - TQ}^2} + \sqrt{n} \delta \right)
    2 V_{\max} 
    \\ & = 2 V_{\max} \sqrt{n} \sqrt{\E \norm{\wh{Q} - TQ}^2}
    + 6 n \delta V_{\max} + \E \norm{f - TQ}^2
    \label{eq:c2step1comb}
  \end{align}
  \begin{lem} Let $a,b>0, \kappa \in (0,1]$ then
    \[ a^2 \leq 2ab + c \implies a^2 \leq (1 + \kappa)^2 b^2 / \kappa
    + (1 + \kappa) c \]
    \label{lem:abc}
  \end{lem}
  \begin{proof} $0 \leq (x - y)^2 = x^2 + y^2 - 2xy \implies 2xy \leq x^2 + y^2$
    for any $x, y \in \R$ so
    \begin{align*}
      2ab & = 2 \sqrt{\frac{\kappa}{1+\kappa}} a \sqrt{\frac{1+\kappa}{\kappa}} b
      \\ & \leq \frac{\kappa}{1+\kappa} a^2 + \frac{1 + \kappa}{\kappa} b^2
    \end{align*}
  \end{proof}
  By \cref{lem:abc} applied to \cref{eq:c2step1comb}
  \begin{align}
    \frac{1}{n} \E \norm{\wh{Q} - TQ}^2
    & \leq \frac{(1+\kappa)^2}{\kappa} \frac{1}{n} V_{\max}^2
    + (1 + \kappa) \left( 6 \delta V_{\max}
    + \frac{1}{n} \E \norm{f - TQ}^2 \right)
    \label{eq:c2step1final0}
  \end{align}
  Since this holds for any $f \in \Cal{F}$ we can further say
  \begin{align}
    \frac{1}{n} \E \norm{\wh{Q} - TQ}^2
    & \leq \frac{(1+\kappa)^2}{\kappa} \frac{1}{n} V_{\max}^2
    + (1 + \kappa) \left( 6 \delta V_{\max}
      + \inf_{f \in \Cal{F}}
    \frac{1}{n} \E \norm{f - TQ}^2 \right)
    \\ & \leq \frac{(1+\kappa)^2}{\kappa} \frac{1}{n} V_{\max}^2
    + (1 + \kappa) \left( 6 \delta V_{\max}
    + \omega(\Cal{F}) \right)
    \label{eq:c2step1final}
  \end{align}
  Where we take the supremum over $\Cal{G}$ (recall $TQ \in \Cal{G}$).

  \textbf{Step 2} Here we link up $\norm{\wh{Q} - TQ}_{\sigma}^2$ 
  with $\E \frac{1}{n} \norm{\wh{Q} -TQ}^2$.
  First  note that
  \begin{align}
    \abs{ \left( \wh{Q}(x) - TQ(x) \right)^2
    - \left( f_{k^*}(x) - TQ(x) \right)^2 }
    &= \abs{ \wh{Q}(x) - f_{k^*}(x) } \cdot
    \abs{ \wh{Q}(x) + f_{k^*}(x) - 2 TQ(x) }
    \\ & \leq 4 V_{\max} \delta
  \end{align}
  Using this twice we can say
  \begin{align}
    & (\wh{Q}(\wh{X}_i) - TQ(\wh{X}_i)^2 
    \\ \leq {} & (\wh{Q}(\wt{X}_i) - TQ(\wt{X}_i))^2
    - (f_{k^*}(\wt{X}_i) - TQ(\wt{X}_i))^2
    + (f_{k^*}(\wt{X}_i) - TQ(\wt{X}_i))^2 
    \\ \leq {} & (f_{k^*}(\wt{X}_i) - TQ(\wt{X}_i))^2 
    + (\wh{Q}(X_i) - TQ(X_i))^2
    - (\wh{Q}(X_i) - TQ(X_i))^2
    \notag \\ & + (f_{k^*}(X_i) - TQ(X_i))^2
    - (f_{k^*}(X_i) - TQ(X_i))^2 + 4 V_{\max} \delta
    \\ \leq {} & (\wh{Q}(X_i) - TQ(X_i))^2
    + (f_{k^*}(\wt{X}_i) - TQ(\wt{X}_i))^2
    - (f_{k^*}(X_i) - TQ(X_i))^2
    + 8 V_{\max} \delta
    \label{eq:c2step2long1}
  \end{align}
  Thus we get
  \begin{align}
    & \norm{\wh{Q} - TQ}_\sigma^2
    \\ = {} & \E \frac{1}{n} \sum_{i=1}^n (\wh{Q}(\wt{X}_i) - TQ(\wt{X}_i))^2
    \\ \leq {} & \E \frac{1}{n} \sum_{i=1}^n \left( (\wh{Q}(X_i) - TQ(X_i))^2 
      + (f_{k^*}(\wt{X}_i) - TQ(\wt{X}_i))^2
    - (f_{k^*}(X_i) - TQ(X_i))^2 \right)
    + 8 V_{\max} \delta 
    \\ = {} & \frac{1}{n} \norm{\wh{Q} - TQ}^2
    + \frac{1}{n} \sum_{i=1}^n h_{k^*}(X_i, \wt{X}_i)
    + 8 V_{\max} \delta
    \label{eq:hfirst}
  \end{align}
  Where we define
  \begin{equation}
    h_j(x, y) \defeq \left( f_j(y) - TQ(y) \right)^2
    - \left( f_j(x) - TQ(x) \right)^2
    \label{eq:c2step2hdef}
  \end{equation}
  For any $j \in [N_\delta]$.
  Define $\Yp = 2 V_{\max}$ and
  \begin{equation}
    T \defeq \max_{j \in [N_\delta]}
    \abs{\sum_{i=1}^n h_j(X_i, \wt{X}_i)/ \Yp}
  \end{equation}
  Then we can bound the middle term in \cref{eq:hfirst}
  \begin{align}
    \E \left( \frac{1}{n} \sum_{i=1}^n h_{k^*}(X_i, \wt{X}_i) \right)
    \leq & \Yp/n \E \max_{j \in [N_\delta]} \left(
    \abs{\sum_{i=1}^n h_j(X_i, \wt{X}_i)/\Yp} \right)
    \\ \leq & \Yp/n \E T 
    \label{eq:havgbound}
  \end{align}
  We want to use Bernsteins inequality (\cref{thm:Bernstein}) with
  $U_i = h_j(X_i,\wt{X}_i)$. Therefore notice that
  $\abs{h_j} \leq \Yp^2$ and
  \begin{align}
    \Var h_j(X_i, \wt{X}_i) = & 2 \Var \left( f_j(X_i) - TQ(X_i) \right)^2
    \\ \leq & 2 \E \left( f_j(X_i) - TQ(X_i) \right)^4
    \\ \leq & 2 \Yp^4
    \label{eq:hvarbound}
  \end{align}
  so by union bounding for any $u < 6 n \Yp$ we have
  \begin{align}
    \E T = & \int_0^\infty \Prob(T \geq t)
    \\ \leq & u + \int_u^\infty \Prob(T \geq t) \difd t
    \\ \leq & u + \int_u^\infty 2 N_\delta
    \exp \left( \frac{-t^2}{2 \Yp t/3 + 4n \Yp^2} \right) \difd t
    \\ \leq & u + 2 N_\delta \int_u^\infty 
    \exp \left( \frac{-t^2}{2 \Yp^2 (t/(3 \Yp) + 2 n )} \right) \difd t
    \\ \leq & u + 2 N_\delta \left(
      \int_u^{6 n \Yp} \exp \left( \frac{-t^2}{8 n \Yp^2} \right) \difd t
      + \int_{6 n \Yp}^\infty \exp \left( \frac{-t}{4/3 \Yp} \right) \difd t
    \right) 
    \label{eq:prenormtail}
    \\ \leq & u + 2 N_\delta \left(
      \frac{8 n \Yp}{2 u} \exp\left( \frac{-u^2}{8 n \Yp} \right)
      + \frac{4 \Yp}{3} \exp\left( \frac{-24 n \Yp}{3 \Yp} \right)
    \right) 
    \label{eq:ETboundu}
  \end{align}
  where we use \cref{lem:normtail}
  from \cref{eq:prenormtail} to \cref{eq:ETboundu}.
  Now set $u = \Yp \sqrt{8 n \log N_\delta}$
  continuing from \cref{eq:ETboundu} we have
  \begin{align}
    \dots = & \Yp \sqrt{8 n \log N_\delta}
    + \frac{\Yp^2 8 n N_\delta}{\Yp \sqrt{8 n \log N_\delta}}
    \exp(-\log N_\delta) + 8/3 N_\delta \Yp \exp(-9/2 n)
    \\ = & \Yp 2 \sqrt{2n} \left( \log N_\delta + \frac{1}{\log N_\delta} \right)
    + 8/3 N_\delta e^{-9/2 n}
    \\ \leq & 4 \sqrt{2} \Yp \sqrt{n \log N_\delta} + 8/3 \Yp
    \label{eq:ETbound}
  \end{align}
  Inserting \cref{eq:ETbound} and \cref{eq:c2step1final} into \cref{eq:hfirst}
  \begin{align}
    \norm{\wh{Q} - TQ}^2_\nu \leq & \frac{1}{n} \E \norm{\wh{Q} - TQ}^2
    + 8 \sqrt{2} V_{\max} n^{-1/2} \sqrt{\log N_\delta}
    + 8 V_{\max} (n^{-1} + \delta)
    \\ \leq & \frac{(1+\kappa)^2}{\kappa} \frac{1}{n} V_{\max}^2
    + (1 + \kappa) \left( 6 \delta V_{\max}
    + \omega(\Cal{F}) \right) 
    \notag
    \\ & + 8 \sqrt{2} V_{\max} n^{-1/2} \sqrt{\log N_\delta}
    + 8 V_{\max} (n^{-1} + \delta)
    \label{eq:C2final}
  \end{align}
\end{proof}



