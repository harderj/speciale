
\subsubsection{Differences in notation}
% Notational differences between this paper
% and [YangXieWang]

Because $\sigma$ is used ambigously in \cref{thm:main}
we denote the probability distribution $\sigma$
from \ncite{F20} p. 20 by $\nu$ instead.
I avoid the shorthand defined in
\ncite{F20} p. 26 bottom:
$\norm{f}_n^2 = 1/n \cdot \sum_{i=1}^n f(X_i)^2$.
and use $p$-norms instead.
The conversion to the notation used here becomes
$\norm{f}_n \leadsto \norm{f}/n$.
The letter $r$ is used in \ncite{F20} to denote the euclidean dimension of
the state space, while here we use $w$.

\subsubsection{The decision model}

\begin{sett}[Fan et al.]
  \leavevmode
  \begin{enumerate}
    \item We're considereing an MDP (\cref{sett:MDP}).
      That is a state and action space
      $(\Cal{S}, \Cal{A})$ and a transition and reward kernel $P, R$
      which only depends on the previous state-action pair.
    \item $S \subseteq \R^w$ is a compact subset of a euclidean space.
    \item $\Cal{A}$ is finite.
    \item Discounted factor satisfy $0 < \gamma < 1$.
  \end{enumerate}
\end{sett}

%todo establish connection to Bertsekas model, e.g. is this u.s.c?


\subsubsection{ReLU Networks}
\begin{defn}[Sparse ReLU Networks]
  For $s,V \in \R$ a (s,V)-\defemph{Sparse ReLU Network} is an ANN $f$
  with all activation functions being \emph{ReLU}
  i.e. $\sigma_{ij} = \max(\cdot, 0)$
  and with weights $(W_\ell, v_\ell)$ satisfying
  \begin{multicols}{3}
    \begin{itemize}
      \item $\max_{\ell \in [L+1]} \norm{\widetilde{W}_\ell}_\infty \leq 1$
      \item $\sum_{\ell = 1}^{L+1} \norm{\widetilde{W}_\ell}_0 \leq s$
      \item $\max_{j \in [d_{L+1}]} \norm{f_j}_\infty \leq V$
    \end{itemize}
  \end{multicols}
  Here $\widetilde{W}_\ell = (W_\ell, v_\ell)$.
  The set of them we denote $\Cal{F}\left(L, \{d_i\}_{i=0}^{L+1},s,V \right)$.
  \label{def:sparseReLU}
\end{defn}
The idea to work with this particular subclass of neural networks come from
\ncite{SH17}, which establishes the following lemma

\begin{lem}[Approximation of HÃ¶lder Smooth Functions by ReLU networks]
    Let $m,M \in \Z_+$ with $N \geq \max\{(\beta + 1)^r, (H + 1) e^r\}$,
    $L = 8 + (m + 5) (1 + \ceil{\log_2(r + \beta)})$, 
    $d_0 = r, d_j = 6(r + \ceil{\beta}) N, d_{L+1} = 1$.
    Then for any $g \in \Cal{C}_r \left( [0,1]^r, \beta, H \right)$
    there exists a ReLU network
    $f \in \Cal{F}\left(L, \{d_j\}_{j=0}^{L+1}, s, \infty \right)$
    with $s \leq 141 (r + \beta + 1)^{3 + r} N (m+6)$
    such that
    \begin{equation*}
      \norm{f - g}_\infty \leq (2 H + 1) 6^r N (1 + r^2 + \beta^2) 2^{-m}
      + H 3^{\beta} N^{-\beta/r}
    \end{equation*}
    \label{lem:holderapprox} 
    %todo: decide whether to do this proof
  \end{lem} 
  \vspace*{-\baselineskip}

\subsubsection{Fitted Q-Iteration}
The algorithm analysed by [Fan et al] is
\begin{figure}[H]
\begin{algorithm}[H] %\label{algocf:fq} % this labels line, could not fix
  \caption{Fitted Q-Iteration Algorithm}
  \KwIn{MDP $(\Cal{S}, \Cal{A}, P, R, \gamma)$, function class $\Cal{F}$,
    sampling distribution $\nu$, number of iterations $K$,
  number of samples $n$, initial estimator $\widetilde{Q}_0$}
  \For{$k = 0,1,2,\dots,K-1$}{
    Sample i.i.d. observations $\{(S_i, A_i), i \in [n]\}$ from $\nu$
    obtain $R_i \sim R(S_i, A_i)$ and $S'_i \sim P(S_i, A_i)$ \\
    Let $Y_i = R_i + \gamma \cdot \max_{a \in \Cal{A}} \widetilde{Q}_k(S'_i, a)$ \\
    Update action-value function:
    \[ \widetilde{Q}_{k+1} \leftarrow
      \argmin_{f \in \Cal{F}} \frac{1}{n}
    \sum_{i=1}^n (Y_i - f(S_i, A_i))^2 \]
  }
  Define $\pi_K$ as the greedy policy w.r.t. $\widetilde{Q}_K$ \\
  \KwOut{An estimator $\widetilde{Q}_K$ of $Q^*$ and policy $\pi_K$}
  \label{alg:fqi}
\end{algorithm}
\end{figure}


