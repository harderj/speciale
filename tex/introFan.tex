This section is about the results of \mcite{F20},
which we will present, discuss and prove.
Similar to the linear function approximation
(see \cref{sec:linearFunctionApprox}), in deep Q-learning
we use a class of functions parametrized by some set $\Theta \subseteq \R^D$.
This time the function class is not linear combinations of a set of
basis functions, but a class of artificial neural networks.
Also we use the same setting (\cref{sett:MR}) of a
continuous state space, finite action space discounted MDP.
Though \ncite{F20} claims to investigate the deep Q-network algorithm,
instead of analysing DQN,
another called \emph{deep fitted Q-iteration} (DQI) algorithm is
analysed instead and bounds on its convergence is established.

We begin by presenting the general \emph{fitted Q-iteration} (FQI) algorithm
on which DQI is based:
\begin{figure}[H]
\begin{algorithm}[H] %\label{algocf:fq} % this labels line, could not fix
  \caption{Fitted Q-Iteration Algorithm}
  \KwIn{MDP $(\Cal{S}, \Cal{A}, P, R, \gamma)$, function class $\Cal{F}$,
    sampling distribution $\nu \in \cl{P}(\cl{S} \times \cl{A})$,
    number of iterations $K$,
    batch-size $n$, initial estimator $\widetilde{Q}_0$
  }
  \For{$k = 0,1,2,\dots,K-1$}{
    Sample $n$ times independently from the distribution $\nu$ to get the batch
    $(S_i, A_i)_{i \in [n]}$.
    \label{line:batchsample}
    \\ For each $i \in [n]$ sample a reward
    $R_i \sim R(S_i, A_i)$ and a next-state $S'_i \sim P(S_i, A_i)$.
    \\ From this define the T-values $Y_i \leftarrow
    R_i + \gamma \cdot \max_{a \in \Cal{A}} \widetilde{Q}_k(S'_i, a)$
    \\ Update action-value function
    by solving the least squares optimization problem
    $ \widetilde{Q}_{k+1} \leftarrow
      \argmin_{f \in \Cal{F}} 
    \sum_{i=1}^n (Y_i - f(S_i, A_i))^2 $
    over the function class $\cl{F}$.
    \label{line:fqioptimize}
  }
  Define $\wt{\tau}_K$ as the greedy policy w.r.t. $\widetilde{Q}_K$ \\
  \KwOut{An estimator $\widetilde{Q}_K$ of the optimal value function $Q^*$
  and an estimator of the optimal policy $\wt{\tau}_K$}
  \label{alg:fqi}
\end{algorithm}
\end{figure}
\begin{rem}
  Rather than a single algorithm we may view
  FQI as a class of concrete, because
  \begin{enumerate*}[label=(\arabic*.)]
    \item The function class is not specified.
    \item It is not specified how to solve the optimization problem
      in \cref{line:fqioptimize}.
  \end{enumerate*}
  These two points are linked in these that the optimization problem
  is probably better handled separately for each function class.
\end{rem}

The deep fitted Q-iteration algorithm is an FQI-algorithm where the
function class $\cl{F}$ is a particular class of artificial neural networks
which we will define now.

\subsubsection{ReLU Networks}
Let $f \in \cl{RN}\left((d_i)_{i=1}^{L+1} \right)$ be a
ReLU network (see \cref{defn:relu})
with weights $(W_i, v_i)_{i=1}^{L+1}$
and define $\wt{W}_i = (W_i, v_i)$, 
as the vector containing all weight and bias parameters of the $i$th layer
of $f$,
that is, all entries in the matrix $W_i$ and vector $v_i$.
We can then consider the magnitude of the maximum parameter
$\norm{\wt{W}_i}_\infty$,
and the number of non-zero parameter $\norm{\wt{W}_i}_0$
in the $i$th layer of the network.
Also denote by $(f_j)_{j \in d_{L+1}} = f$ the components (coordinates)
of the network $f$.

\begin{defn}[Sparse ReLU networks]
  For $s,V \in \R$ the ReLU network $f$
  is called $(s,V)$-\defemph{sparse} if
  \begin{center}
    \begin{enumerate*}[label=\arabic*., itemjoin=\hspace{0.3in}]
      \item $\max_{\ell \in [L+1]} \norm{\widetilde{W}_\ell}_\infty \leq 1$
      \item $\sum_{\ell = 1}^{L+1} \norm{\widetilde{W}_\ell}_0 \leq s$
      \item $\max_{j \in [d_{L+1}]} \norm{f_j}_\infty \leq V$
    \end{enumerate*}
  \end{center}
  The set of them we denote $\Cal{SRN}\left(s, V, (d_i)_{i=0}^{L+1}, L \right)$
  and by $\cl{SRN}(s, V)$ we mean the set of $(s, V)$-sparse ReLU networks
  with any (finite) structure.
  We may leave out $L$ when clear from the structure writing
  $\cl{SRN}\left(s, V, (d_i)_{i=0}^{L+1} \right)$.
  \label{defn:sparseReLU}
\end{defn}

\begin{rem}
  Following the graph interpretation of ANNs (see \cref{rem:annGraph})
  the condition that
  $\sum_{i=1}^{L+1} \norm{\wt{W}_i}_0 \leq s$
  corresponds to graph-theoretical sparsity of the graph derived from
  the ANN.
\end{rem}

\begin{defn}[Deep fitted Q-iteration]
  A \defemph{deep fitted Q-iteration} (DQI) algorithm,
  is the fitted Q-iteration algorithm when applied with a function class
  of sparse ReLU networks $\cl{SRN}$.
\end{defn}

The reason for working with this particular subclass of neural networks
is due to the following lemma found in
\mcite{SH17} p. 22 (we have not yet defined Hölder smooth functions.
For this see \cref{defn:holdersmooth}).

\begin{lem}[Approximation of Hölder Smooth Functions by ReLU networks]
  Let $m,M \in \Z_+, \beta > 0$ and $H>0$
  with $N \geq \max\{(\beta + 1)^r, (H + 1) e^r\}$,
  $L = 8 + (m + 5) (1 + \ceil{\log_2(r + \beta)})$, 
  $d_0 = r, d_j = 6(r + \ceil{\beta}) N, d_{L+1} = 1$.
  Then for any $g \in \Cal{C}_r \left( [0,1]^r, \beta, H \right)$
  there exists a ReLU network
  $f \in \cl{SRN}\left(s, \infty, (d_j)_{j=0}^{L+1} \right)$
  with $s \leq 141 (r + \beta + 1)^{3 + r} N (m+6)$
  such that
  \begin{equation*}
    \norm{f - g}_\infty \leq (2 H + 1) 6^r N (1 + r^2 + \beta^2) 2^{-m}
    + H 3^{\beta} N^{-\beta/r}
  \end{equation*}
  \label{lem:holderapprox} 
    %todo: decide whether to do this proof
\end{lem} 
\vspace*{-\baselineskip}

In the course of establishing the results in \ncite{F20} we will not go
more into this result or other properties of ReLU networks in particular,
instead putting emphasis on how to use this result to obtain the main
theorem.

\subsection{Assumptions}

Before we present the main result of \ncite{F20}
we will first properly state the rather
intricate assumptions that it requires.

\subsubsection{Hölder Smoothness}
According to \ncite{F20} (def. 2.2) the following property is
\emph{widely used as to characterize regularity of functions}.
\begin{defn}[Hölder smoothness]
  Let $\cl{S}$ be subset of $\R^w$ with non-empty interior
  $\cl{S}^\circ \neq \emptyset$
  (see \cref{defn:interior}),
  $\beta > 0$ be a real number, $k = \floor{\beta} \in \N_0$
  and $f : \Cal{S} \to \R \in C^k$ be a $k$ times continuously
  differentiable function (see \cref{defn:diffRn}). Define
  the \defemph{Hölder smooth norm} of $f$ by
  \begin{equation}
    \norm{f}_{C_w} \defeq 
    \sum_{|{\alpha}| < \beta}
    \norm{\partial^{\alpha}f}_\infty +
    \sum_{\norm{{\alpha}}_1 = \floor{\beta}}
    \sup_{\substack{x \neq y \\ x, y \in \cl{S}^\circ}}
    \frac{|\partial^\alpha (f(x) - f(y))|}
  {\norm{x-y}_\infty^{\beta-\floor{\beta}}}
  \end{equation}
  where $\alpha = (\alpha_1, \dots, \alpha_w) \in \N_0^w$.
  If $\norm{f}_{C_w} < \infty$ then $f$ is \defemph{Hölder smooth}.
  Given a compact subset $\Cal{D} \subseteq \R^w$
  the space of Hölder smooth functions on $\Cal{D}$ with norm bounded by
  $H > 0$ is denoted
  \[ C_w(\Cal{D}, \beta, H) \defeq
  \left\{ f : \Cal{D} \to \R \Mid \norm{f}_{C_w} \leq H \right\} \]
  \label{defn:holdersmooth}
\end{defn}

With this we can define the criteria we are actually interested in
using

\begin{defn}
  For any $j \in [q]$ let $t_j, p_j \in \N$, $t_j\leq p_j$ and $H_j, \beta_j > 0$.
  We say that $f:[a_1, b_1]^{p_1} \to \R$
  is a \defemph{composition of Hölder smooth functions} when
  \[ f = g_q \circ \dots \circ g_1 \]
  for some functions $g_j : [a_j, b_j]^{p_j} \to [a_{j+1}, b_{j+1}]^{p_{j+1}}$
  (where $p_{q+1} = 1$)
  that only depend on $t_j$ of their inputs
  for each of their components $g_{jk}:[a_j, b_j]^{p_j} \to [a_{j+1}, b_{j+1}]$,
  and satisfies $g_{jk} \in C_{t_j}([a_j, b_j]^{t_j}, \beta_j, H_j)$, 
  i.e. they are Holder smooth.
  We denote the class of these functions
  \[ \Cal{G}(\{p_j, t_j, \beta_j, H_j\}_{j \in [q]}) \]
\end{defn}

\begin{example}
  We have for example $\cl{G}(w, w, \beta, H) = C_w([a_1, b_1]^w, \beta, H)$.
\end{example}

\begin{defn}
  Define
  \[ \Cal{F}_0 = \left\{ f : \Cal{S} \times \Cal{A} \to \R \Mid
  \forall a \in \Cal{A} : f(\cdot, a) \in \Cal{SRN}(s, V) \right\} \]
  and
  \[ \Cal{G}_0 = \left\{ f : \Cal{S} \times \Cal{A} \to \R
      \Mid \forall a \in \Cal{A} :
      f(\cdot, a) = \Cal{G}(\{p_j, t_j, \beta_t, H_j\}_{j \in [q]})
  \right\} \]
  Here $\cl{SRN}(s, V)$ denotes the set of $(s, V)$-sparse ReLU networks
  with any possible network structure.
  \label{defn:F0G0}
\end{defn}

The class $\cl{F}_0$ is the function class we are going to use in the version
of the DQI algorithm, for which we will soon establish convergence bounds.

In order to make sense of the first assumption (\cref{asm:A1}) which we
are going to present shortly
we recall here the definition of the operators for Q-functions:
(\cref{defn:opQ}). For any stationary policy $\tau \in S\Pi$ we define
\[ P_\tau Q(s, a) = \int Q(s', a') \difd \tau P(s', a' \mid s, a) \]
\[ T_\tau Q = r + \gamma P_\tau Q \]
\[ T Q(s, a) = r(s, a) + \gamma
\int \max_{a' \in \Cal{A}} Q(s', a') \difd P(s' \mid s, a) \]
matching the definitions in \ncite{F20}.

\begin{asm}
  It is assumed $ T \Cal{F}_0 \subseteq \Cal{G}_0$.
  I.e. t is assumed that $T f \in \Cal{G}_0$ for any $f \in \Cal{F}_0$, 
  so when using the Bellman optimality operator on our sparse ReLU networks,
  we should stay in the class of compositions of Holder smooth functions.
  \label{asm:A1}
\end{asm}

If also $\Cal{G}_0$ is well approximated by functions in $\Cal{F}_0$
then this assumption implies that $\Cal{F}_0$ is approximately closed
under the Bellman operator $T$ and thus that $Q^*$ is close to $\Cal{F}_0$.
We now look at a simple example where \cref{asm:A1} holds:
Seting $\Cal{D}=[0,1]^r$, $q=1$ 
and taking both the expected reward function and transition kernel
to be Hölder smooth.

\begin{example}
  Assume for all $a \in \Cal{A}$ that
  $P(\cdot \mid s,a)$ is absolutely continuous w.r.t. $\lambda^k$
  (the $k$ dimensional Lebesgue measure)
  with density $p(\cdot \mid s, a)$,
  that for all $s' \in \Cal{S}$ we have
  $s \mapsto p\left(s' \Mid s, a \right)$
  and $s \mapsto r(s, a)$ are both Hölder smooth in the class
  $C_w([0,1]^w, \beta, H)$.
  Then
  \[ T \Cal{F}_0 \subseteq C_w([0,1]^w, \beta, (1 + \gamma V_{\max}) H)
  \subseteq \cl{G}_0 \] 
  when $q=1, p_1 = w, t_1 = w, \beta_1 = \beta$ and $H_1 =
  (1 + \gamma V_{\max}) H$.
  To see this let
  Let $f \in \Cal{F}_0$ and $\alpha \in \N_0^w$.
  Observe that
  \begin{align*}
    \partial^\alpha (Tf)(s, a)
    = & \; \partial^\alpha_s \left( r(s, a) \right)
    + \gamma \int_{\Cal{S}}\partial^\alpha_s \left[ \max_{a' \in \Cal{A}}
    f(s', a') p\left(s' \Mid s, a\right) \right] \difd s' 
    \\ \leq & \; \partial^\alpha_s \left( r(s, a) \right)
    + \gamma V_{\max} \sup_{s' \in \Cal{S}} \partial_s^\alpha
    p\left(s' \Mid s, a\right)
  \end{align*}
  similarly
  \begin{align*}
    \partial^\alpha (Tf)(s, a) - \partial^\alpha (Tf)(s', a)
    \leq & \; \partial^\alpha_s \left( r(s, a) \right)
    - \partial^\alpha_s \left( r(s', a) \right)
    \\ & \; + \gamma V_{\max} \sup_{s'' \in \Cal{S}}
    \left( \partial_s^\alpha p(s'' \Mid s, a)
    - \partial_s^\alpha p(s'' \Mid s', a) \right)
  \end{align*}
  Thus since $p$ and $r$ are Hölder smooth
  \begin{align*}
    \norm{Tf}_{C_w} \leq & \; \sum_{\abs{\alpha}<\beta} \left(
      \norm{\partial^\alpha r(\cdot, a)}_\infty
      + \gamma V_{\max} \sup_{s \in \Cal{S}} \norm{\partial^\alpha
    p(s \mid \cdot, a)}_\infty \right)
    \\ + & \; \sum_{\norm{\alpha}_1 = \floor{\beta}} \sup_{x \neq y}
    \left(
      \frac{\abs{\partial^\alpha (r(x, a) - r(y, a))}}
      {\norm{x - y}_{\infty}^{\beta - \floor{\beta}}}
      + \gamma V_{\max} \sup_{s \in \Cal{S}} \frac{
      \abs{\partial^\alpha (p(s \mid x, a) - p(s \mid y, a))}}
      {\norm{x - y}_{\infty}^{\beta - \floor{\beta}}}
    \right)
    \\ \leq & \; H + \gamma V_{\max} H = (1 + \gamma V_{\max}) H
  \end{align*}
  
\end{example}

\subsubsection{Concentration coefficients}

In analysing DQI we will work with two distributions (measures) on $\cl{S}
\times \cl{A}$.
The first measure $\nu$ is the batch sampling distribution
used in the FQI algorithm (\cref{line:batchsample}). %todo
The other $\mu$ is used to measure the distance to the optimal Q-function $Q^*$
from the algorithm output $\wt{Q}_K$.
The next assumption has to do with the difference between these two measures.

Since we are in the setting of an MDP $(\cl{S}, \cl{A}, P, R, \gamma)$
recall from chapter 2
that for a sequence of stationary policies $\pi_1, \pi_2, \dots \in S \Pi$ and a
measure $\mu \in \cl{P}(\cl{S} \times \cl{A})$ we can get a probability measure
$\dots \pi_2 P \pi_1 P \in \cl{P}(\cl{H}_\infty)$ where
$\cl{H}_\infty = (\cl{S} \times \cl{A})^\infty$.
Let $\rho_m:\cl{H}_\infty \to \cl{S}$
denote projection onto the $m$th state-action pair in
$\cl{H}_\infty$.
Then using the alternative kernel composition (see \cref{rem:altComp})
we can write the distribution of the $m$th state action pair as
\[\rho_{m}(\dots \pi_2 P \pi_1 P \mu)
= (\pi_{m-1} P) \circ \dots \circ (\pi_1 P) \in \cl{P}(\cl{S}) \]


\begin{defn}[Concentration coefficients] \label{defn:ccoefs}
  Let $\nu_1, \nu_2 \in \Cal{P}(\Cal{S}\times \Cal{A})$ be probability measures,
  absolutely continuous w.r.t. $\lambda^w \otimes \mu_\Cal{A}$
  (the product of the $w$-dimensional Lebesgue measure and the counting measure
  on $\Cal{A}$).
  Define
  \[ \kappa(m, \nu_1, \nu_2) = \sup_{\pi_1, \dots, \pi_m \in M\Pi}
    \left[ \E_{v_2} \left( \frac{\mathrm{d}
	((P \pi_m) \circ \dots \circ (P \pi_1) \nu_1)}
  {\mathrm{d} \nu_2} \right)^2 \right]^{1/2} \]
  where $\frac{\difd \mu_1}{\difd \mu_2}$ are the Radon-Nikodym derivative
  of the measures $\mu_1, \mu_2$ (see \cref{thm:radonNiko}).
\end{defn}

\begin{asm}\label{asm:A2}
  For two probability measure $\nu, \mu \in \cl{P}(\cl{S} \times \cl{A})$ 
  on $\cl{S} \times \cl{A}$
  it is assumed that there exists a finite constant $\phi_{\mu, \nu} > 0$
  such that
  \[ \phi_{\mu, \nu} \defeq 
    (1 - \gamma)^2 \sum_{m\geq 1} \gamma^{m-1} m \kappa(m, \mu, \nu)
  < \infty \]
\end{asm}

We are not going further into examples of when this assumption holds
or the size of the constant $\phi_{\mu, \nu}$.
Below assumption 4.3 in \ncite{F20} are found references to detailed discussions
of \cref{asm:A2}.

\subsection{The main theorem}

The main result of this section and one of the main results of \ncite{F20} is

\begin{thm}[Fan, Yang, Xie, Wang] \label{thm:main}
  Let \cref{sett:MR} hold and let
  $\nu, \mu \in \cl{P}(\cl{S} \times \cl{A})$ be probability
  measures on $\cl{S} \times \cl{A}$ such that
  \cref{asm:A2} hold with constant $\phi_{\mu, \nu} > 0$.
  We will use the FQI with the following inputs:
  \begin{enumerate}
    \item $\nu$ as the batch sampling distribution.
    \item $n \in \N$ a sufficiently
      large\footnote{We will elaborate on this in the proof.}
      number serving as the batch size.
    \item approximating function class $\cl{F}_0$ (see \cref{defn:F0G0}).
  \end{enumerate}
  We assume that there exists constants
  $q \in \N$ and
  $\{p_j, t_j, \beta_j, H_j\}_{j \in [q]}$
  such that \cref{asm:A1} hold and that furthermore
  there exists constant $\xi > 0$
  such that
  \[ \max \left\{ \sum_{j=1}^q (t_j + \beta_j + 1)^{3 + t_k},
      \sum_{j=1}^q \log (t_j + \beta_j),
      \max_{j \in [q]} p_j
  \right\} \leq (\log n)^\xi \] 

  Set $\beta^*_j = \beta_j \prod_{\ell = j+1}^q \min(\beta_\ell, 1)$
  for $j\in [q-1]$, $\beta^*_q = 1$,
  $\alpha^* = \max_{j \in [q]} t_j/(2\beta^*_j + t_j)$, 
  $\xi^* = 1 + 2\xi$ and $\kappa^* = \min_{j\in [q]} \beta^*_j/t_j$.

  Then there exists a class of ReLU networks
  \[ \Cal{F}_0 = \{f : \Cal{S} \times \Cal{A} \to \R : f(\cdot, a) \in 
  \Cal{SRN}(\wt{s}, V_{\max}, (\wt{d}_j)_{j=0}^{\wt{L}+1}, \wt{L})
  \mid a \in \Cal{A} \} \]
  with structure satisfying
  \[ \wt{L} \leq C_{\wt{L}} \cdot (\log n)^{\xi^*},\;
    \wt{d}_0 = r,\; \wt{d}_j \leq 6 n^{\alpha^*} (\log n)^{\xi^*},\;
    d_{L+1} = 1,\;
  \wt{s} \leq C_{\wt{s}} \cdot n^{\alpha^*} \cdot (\log n)^{\xi^*} \]
  such that when running the FQI algorithm, its output satisfy
  \[ \norm{Q^* - Q_{\pi_K}}_{1, \mu} \leq \;
    C_{\varepsilon} \frac{\phi_{\mu, \nu} \gamma}{(1-\gamma)^2} V_{\max}^2
    n^{\max\{-2\alpha^*\kappa^*, (\alpha^*  - 1)/2 \}} \log(n)^{1+2\xi^*}
    + \frac{4 \gamma}{(1-\gamma)^2} R_{\max} \gamma^K
  \]
  here $C_{\varepsilon}, C_{\wt{L}}, C_{\wt{s}}>0$ are constants
  not depending on $n$ or $K$.
  In other words
  \[ \norm{Q^* - Q_{\pi_K}}_{1, \mu} =
  \cl{O}\left( n^{-\ve^*} \log(n)^{c^*} + \gamma^K \right) \]
  for some $\ve^*, c^* > 0$.
\end{thm}

This bound on the convergence of the FQI algorithm
is quite remarkable in terms of class
of environments that it shows can be solved approximatively by using
sampling from the environment to update a ANN-represented Q-function.
In particular it is the most general result on convergence rates for
model-free and continuous state space algorithms,
among the sources we survey in this thesis.

\subsection{Relation to DQN}

The following is famous \emph{DQN}-algorithm proposed by \mcite{M15}.
Note that we denote by $Q(\theta)$ the network with parameters $\theta$,
i.e. $Q(\theta)$ is a function.
\begin{figure}[H]
\begin{algorithm}[H] %\label{algocf:fq} % this labels line, could not fix
  \caption{Deep Q-Network}
  \KwIn{MDP $(\cl{S}, \cl{A}, P, R, \gamma)$, number of iterations $K$,
  batch size $n$, exploration factor $\epsilon$, function class $\cl{F}$
  of deep neural networks
  parametrized by some $\Theta \subseteq \R^D$, $D \in \N$,
  target update frequency $T_{\mathrm{target}}$,
  learning rates $\{\alpha_t\}_{t \geq 0}$}

  Initialize replay memory $\cl{M} \leftarrow \emptyset$ as empty.

  Pick a initial Q-network $\wt{Q}_0 = Q(\theta_0)$ by sampling
  $\theta_0 \in \Theta$ from some distribution.

  Initialize target network $Q_{\mathrm{target}, 0} = \wt{Q}_0$ by picking
  the target parameters $\theta^*_0 = \theta_0$ and setting
  $Q_{\mathrm{target}, 0} = Q(\theta^*_0)$.

  \For{$k = 0,1,2,\dots,K-1$}{
    With probability $\epsilon$ sample $A_k$ uniformly from $\cl{A}$,
    and with probability $1-\epsilon$ choose $A_k$ greedily with respect
    to $\wt{Q}_k$, that is
    $A_k$ is picked from $\argmax_{a \in \cl{A}} \wt{Q}_k(S_k, a)$.

    Sample (observe) $S_{k+1}$ and $R_k$ (from $P(\cdot \mid S_k, A_k)$
    and $R(\cdot \mid S_k, A_k)$).

    Store the transition $(S_k, A_k, R_k, S_{k+1})$ in the replay
    memory $\cl{M}$, potentially replacing an old (random) transition
    if the memory is \emph{full}.

    Experience replay: Sample batch of transitions
    $(s_i, a_i, r_i, s'_i)_{i \in [n]}$ from the replay memory $\cl{M}$.

    For each $i \in [n]$ let $Y_i = r_i + \gamma \max_{a \in \cl{A}}
    Q_{\mathrm{target}, \ell(k)}(s_i', a)$.

    Update the Q-network by performing a gradient descent step
    \[ \theta_{k+1} \leftarrow \theta_k - \alpha_k \frac{1}{n} \sum_{i = 1}^n
	(Y_i - Q(\theta_k)(s_i, a_i)) \cdot \nabla_{\theta}
    Q(\theta)(s_i, a_i) \]

    For every $T_{\mathrm{target}}$ steps update the target network by
    setting $\theta^*_{\ell(k+1)} \leftarrow \theta_{k+1}$
    where $\ell(k)$ is the number of updates of the target network
    at step $k$.
  }
  Put $\wt{Q}_K = Q(\theta_K)$ and pick a greedy policy $\wt{\tau}$
  with respect to $\wt{Q}_K$.

  \KwOut{An estimator $\wt{Q}_K$ of the optimal value function $Q^*$
  and $\wt{\tau}_K$ an estimator of the optimal policy $\tau^*$.}
  \label{alg:DQN}
\end{algorithm}
\end{figure}

DQN is an off-policy algorithm because it updates the
parameter $\theta_k$ based on picking the greedy action of the
target network $Q_{\hrm{target}, \ell(k)}$, while the policy being
followed is an $\epsilon$-greedy policy where the greedy part is
with respect to $\wt{Q}_k$.

\ncite{F20} stresses two \emph{tricks} that drives the succes of DQN,
which is the use of
\begin{enumerate}
  \item \emph{experience replay}
  \item \emph{target network}
\end{enumerate}
Experience replay is the basic method of keeping a replay memory
set (or \emph{buffer}) from which samples (or \emph{mini-batches}) are
drawn which then are used in each gradient descent update of the
Q-network.

The target network is a past version of the Q-network that is used
in the gradient step update as the goal after using the Bellman operator,
It is then updated every $T_{\hrm{target}}$ steps.

In practice the size of the replay memory buffer is very large, for example
in \ncite{M15} it holds $\sim 10^6$ transitions.
Because of this it is argued in \ncite{F20} that
\begin{displayquote}
``experience replay is close to sampling independent transitions
from a given distribution $\sigma \in \cl{P}(\cl{S} \times \cl{A})$''
\end{displayquote}
For the target network it is argued that when having a large enough batch
($n$) and using $\wt{Q}_{k-1}$ to update $\wt{Q}_k$, the role of
$\wt{Q}_{k-1}$ becomes similar to the target network
$Q_{\hrm{target}, \ell(k-1)}$ of DQN.

\subsection{Critique of this relation}

While the arguments for the similarity between FQI and DQN are intuitively
reasonable, the rigiorous proofs are missing and it is unclear if
a convergence result about FQI has implications for DQN.

\subsubsection{Differences in notation}
% Notational differences between this thesis
% and [YangXieWang]

Because $\sigma$ is used ambigously in \cref{thm:main}
we denote the probability distribution $\sigma$
from \ncite{F20} p. 20 by $\nu$ instead.
I avoid the shorthand defined in
\ncite{F20} p. 26 bottom:
$\norm{f}_n^2 = 1/n \cdot \sum_{i=1}^n f(X_i)^2$.
and use $p$-norms instead.
The conversion to the notation used here becomes
$\norm{f}_n \leadsto \norm{f}/n$.
The letter $r$ is used in \ncite{F20} to denote the euclidean dimension of
the state space, while here we use $w$.
We use $\cl{SRN}(s, V, (d_j)_{j=0}^{L+1}, L)$
  to denote the class of sparse ReLU networks while
\ncite{F20} use $\cl{F}(L, \{d_j\}_{j=0}^{L+1}, s, V)$.
