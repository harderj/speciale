

\subsubsection{The decision model}

\begin{sett}[Fan et al.]
  \leavevmode
  \begin{enumerate}
    \item We're considereing an MDP (\cref{sett:MDP}).
      That is a state and action space
      $(\Cal{S}, \Cal{A})$ and a transition and reward kernel $P, R$
      which only depends on the previous state-action pair.
    \item $S \subseteq \R^w$ is a compact subset of a euclidean space.
    \item $\Cal{A}$ is finite.
    \item Discounted factor satisfy $0 < \gamma < 1$.
  \end{enumerate}
\end{sett}

%todo establish connection to Bertsekas model, e.g. is this u.s.c?

\subsubsection{Artificial neural networks}

\begin{defn}\label{def_ANN}
  An \textbf{ANN} (Artificial Neural Network) with structure
  $\{ d_i \}_{i=0}^{L+1} \subseteq \N$,
  activation functions $\sigma_i = (\sigma_{ij} : \R \to \R)_{j=1}^{d_i}$
  and weights $\{ W_i \in M^{d_i \times d_{i-1}}, v_i \in \R^{d_i} \}_{i=1}^{L+1}$
  is the function $F:\R^{d_0} \to \R^{d_{L+1}}$ 
  \[ F = w_{L+1} \circ \sigma_L \circ w_L
  \circ \sigma_{L-1} \circ \dots \circ w_1 \]
  where $w_i$ is the affine function $x \mapsto W_i x + v_i$ for all $i$.

  Here $\sigma_i(x_1, \dots, x_{d_i})
  = (\sigma_{i1}(x_1), \dots, \sigma_{id_{i}}(x_{d_{i}}))$.

  $L \in \N_0$ is called the number of hidden layers.

  $d_i$ is the number of neurons or nodes in layer $i$.
\end{defn}

An ANN is called \emph{deep} if there are two or more hidden layers.

\begin{defn}[Sparse ReLU Networks]
  For $s,V \in \R$ a (s,V)-\defemph{Sparse ReLU Network} is an ANN $f$
  with all activation functions being \emph{ReLU}
  i.e. $\sigma_{ij} = \max(\cdot, 0)$
  and with weights $(W_\ell, v_\ell)$ satisfying
  \begin{multicols}{3}
    \begin{itemize}
      \item $\max_{\ell \in [L+1]} \norm{\widetilde{W}_\ell}_\infty \leq 1$
      \item $\sum_{\ell = 1}^{L+1} \norm{\widetilde{W}_\ell}_0 \leq s$
      \item $\max_{j \in [d_{L+1}]} \norm{f_j}_\infty \leq V$
    \end{itemize}
  \end{multicols}
  Here $\widetilde{W}_\ell = (W_\ell, v_\ell)$.
  The set of them we denote $\Cal{F}\left(L, \{d_i\}_{i=0}^{L+1},s,V \right)$.
  \label{def:sparseReLU}
\end{defn}

\subsubsection{Fitted Q-Iteration}
The algorithm analysed by [Fan et al] is
\begin{figure}[H]
\begin{algorithm}[H] %\label{algocf:fq} % this labels line, could not fix
  \caption{Fitted Q-Iteration Algorithm}
  \KwIn{MDP $(\Cal{S}, \Cal{A}, P, R, \gamma)$, function class $\Cal{F}$,
    sampling distribution $\nu$, number of iterations $K$,
  number of samples $n$, initial estimator $\widetilde{Q}_0$}
  \For{$k = 0,1,2,\dots,K-1$}{
    Sample i.i.d. observations $\{(S_i, A_i), i \in [n]\}$ from $\nu$
    obtain $R_i \sim R(S_i, A_i)$ and $S'_i \sim P(S_i, A_i)$ \\
    Let $Y_i = R_i + \gamma \cdot \max_{a \in \Cal{A}} \widetilde{Q}_k(S'_i, a)$ \\
    Update action-value function:
    \[ \widetilde{Q}_{k+1} \leftarrow
      \argmin_{f \in \Cal{F}} \frac{1}{n}
    \sum_{i=1}^n (Y_i - f(S_i, A_i))^2 \]
  }
  Define $\pi_K$ as the greedy policy w.r.t. $\widetilde{Q}_K$ \\
  \KwOut{An estimator $\widetilde{Q}_K$ of $Q^*$ and policy $\pi_K$}
  \label{alg:fqi}
\end{algorithm}
\end{figure}


