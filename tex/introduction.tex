
\subsection{Reinforcement Learning}

RL is a broad topic and a main branch of
machine learning. Because of its broadness it overlaps with other disciplines
such as control theory and dynamic programming.

In Reinforcement Learning, as in dynamic programming,
we are concerned with finding an optimal policy
for an agent in some environment.
This environment is described by
a sequence of state and action spaces
$\Cal{S}_1, \Cal{A}_1, \Cal{S}_2, \dots$
and rules (or dynamics) formalized as probability kernels
$P_1, R_1, P_2, \dots$ specifying which states and rewards
are likely to follow after some action is chosen.
One can then specify rules $\pi$, called a \emph{policy},
for how the agent should choose actions in every situation in the environment.
Given an environment and a policy one obtains a stochastic process,
that is a distribution on sequences of states, actions and
rewards.
One can then measure the performance of the policy by looking at
the expected accumulated rewards called the \emph{policy evaluations}
$V_\pi$ of the policy.
The goal of reinforcement learning is to find an optimal policy $\pi^*$,
maximizing the value function.

$V_\pi$ is viewed as function that evaluates for each \emph{starting state}
$s \in \cl{S}_1$ the expected accumulated rewards when starting in state $s$
and following policy $\pi$.
There might therefore be different optimal policies for each such starting
state.
Traditionally one defines an optimal value function $V^*(s)$
by taking supremum over all policies $\sup_\pi V_\pi(s)$ for every state
$s \in \cl{S}_1$.
Then an optimal policy $\pi^*$ should satisfy $V_{\pi^*} = V^*$,
i.e. it should be optimal uniformly across all starting states $\cl{S}_1$.
The existence of optimal policies defined in this way is a non-trivial
question and we will devote some time on this.

A particular class of environments which are called Markov decision processes
(MDPs).
In an MDP the same state space $\cl{S}$, action space $\cl{A}$ and rules
$P, R$ are used throughout the process.
They are by far the most well-studied environments.
With an MDP and a value function $V_1$ satisfying certain assumptions 
one can obtain a policy $\pi_1$ by choosing actions
leading to the maximum average values (according to $V_1$).
Such policies are called \emph{greedy policies}.
We can then evaluate the policy $\pi_1$
yielding a new value function $V_2 = V_{\pi_1}$.
The process of evaluating policies and picking greedy policies
is formalised by so called \emph{T-operators} $T_\pi, T$.
One of these ($T$) is called the \emph{Bellman optimality operator}
and combines policy evaluation and greedy choices.
This process of applying the $T$ operators and picking greedy policies
can be continued indefinitely yielding a sequence of value
functions and policies.
Variations of this idea are called \emph{value iteration} and
\emph{policy iteration},
and is derived from dynamic programming.
We show that value iteration converges to the optimal value functions
given mild assumptions on the MDP.
Furthermore we show that the optimal value functions is a fixed point
of the Bellman optimality operator: $TV^* = V^*$
This is called the \emph{Bellman optimality equation} and
is central to all problems in dynamic programming.

We have now described the roots of RL in dynamic programming.
However RL usually refers to algorithms that
are not merely value iterations, but instead work without
directly using the transition and reward dynamics,
and instead estimate value functions based only on sampling from the
environment. Such algorithms are called \emph{model-free}.
We will not look at algorithms which are based on sampling, and instead focus
on theoretical aspects assuming it is possible to use the transition dynamics
directly.

\subsection{Q-learning}

A problem with value functions defined on the set of states $\cl{S}$ is that
picking optimal actions require knowledge of the transition dynamics $P$.
This is especially a problem for model-free algorithms.
To get around this problem \emph{Q-functions} were introduced, which evaluates
the value of a state-action pair, instead of only a state.

Given a Q-function $Q$, picking best actions according to $Q$ now
merely require maximization over $Q$ itself.
Also it turns out that Q-functions is
more convenient to work with computationally.
In this paper we show that value and policy iteration can be done
for Q-functions in a virtually identical manner, when the process dynamics
are known.

When the process dynamics are hidden designing algorithms becomes trickier.
In such settings approaches to the problem
fall in two categories. In the \emph{indirect} approaches
one attempts to estimate the process dynamics first and then afterwards
methods for the known-dynamics are applied.
The \emph{direct} approaches basically covers \emph{the rest}.
In the direct category we find the popular \emph{temporal difference}
algorithms on which \emph{fitted Q-iteration} (FQI)
and the \emph{deep Q-network} (DQN) algorithm of \ncite{M15} is based.
Many direct approaching such as FQI and DQN can be seen as
stochastic approximations of the Bellman optimality equation.

\emph{Q-learning} is the category of algorithms that iteratively updates
Q-functions in the attempt to improve the derived policy.
\emph{Deep} Q-learning is then the subcategory of algorithms which
uses deep neural networks as approximators for the Q-functions.
We will see in this paper how Q-functions are used to find optimal
policies (strategies) for decision processes and how they work
as the underlying \emph{knowledge} that drives the decisions of
the agent. We will use a wide array of function classes in the attempt
to approximate ideal Q-functions such as the policy evaluations and
optimal Q-functions.

All this will be made precise in the next section.
Before proceeding to this we include a brief introduction to the basic
concept and notation we are going to use throughout the paper.

\subsection{Basic concepts and notation}

The real numbers $\R$ is endowed with
the standard ordering with
giving rise to the
standard order topolog
(\cref{defn:orderTop}).
This in turn give rise to the standard Borel $\sigma$-algebra
(\cref{defn:BorelAlg}) $\bb{B} = \sigma(\cl{O})$
generated by the open sets $\cl{O}$ of the standard topology on $\R$.

When considering a measurable space $\cl{X}$ 
we always denote its $\sigma$-algebra
$\Sigma_\cl{X}$ when not ambiguous.
We always
consider the cartesian product of measurable spaces
with the product $\sigma$-algebra (\cref{defn:prodSigmaAlg})
unless otherwise specified.
We denote the set of measurable functions (\cref{defn:measFunc})
$\cl{X} \to \cl{Y}$ between two measurable spaces by 
$\cl{M}(\Sigma_\cl{X}, \Sigma_\cl{Y})$ or $\cl{M}(\cl{X}, \cl{Y})$
when the $\sigma$-algebras are not ambiguous
or simply $\cl{M}(\cl{X})$ when $\cl{Y} = \R$.

The set of probability measures on $\cl{X}$ is denoted
$\cl{P}(\Sigma_\cl{X})$ or $\cl{P}(\cl{X})$ when $\Sigma_\cl{X}$ is implicit
(not to be confused with the powerset of $\cl{X}$
which we denote $2^{\cl{X}}$).

When talking about functions $f_1, f_2, \dots : \cl{X} \to \R$
limits are always understood pointwise, unless otherwise stated,
meaning that $f_n \to f$ is to be read as
$\forall x \in \cl{X} : f_n(x) \to f(x)$.
The same goes for logical operators, e.g. $f > 0$ is to be understood
as $f(x) > 0$ for all $x \in \cl{X}$.


