In this thesis we give an introduction to Q-learning and discuss
convergence results of \emph{Q-learning} algorithms from its beginning in
1989 \ncite{W89} to a result obtained in the preprint \mcite{F20}.
The introduction includes fundamental theory of the underlying field of
\emph{dynamic programming} and \emph{reinforcement learning}
and related topics such as \emph{value iteration}.

\section{Motivation}
The topic was inspired by the performance of the algorithms
implemented by \mcite{M15}.
In \ncite{M15} it was shown how a single algorithm was able to achieve super
human performances in a variety of problems, namely playing Atari 2600 video
games, only using raw pixels and a reward (score) as input and a large number of
interactions with the environment.
The algorithm used in \ncite{M15} is based on a Q-learning
algorithm called the \emph{deep Q-network} (DQN) algorithm.

The purpose of this thesis was initially 
to investigate what has been proven
about the convergence of the DQN algorithm
and what mathematical theory is relevant to establish such proofs.

The work began by considering the preprint
\mcite{F20} which claims to establish theoretical justification for
the convergence of DQN to \emph{the optimal Q-function}.

In the course of reading \ncite{F20} it became clear that the background
context of dynamic programming, reinforcement learning and value iteration
was essential to understand the results of \ncite{F20} and similar papers
and also compare such results.
Also questions as to in which settings optimal policies exists turned out
to be a non-trivial question.
Therefore in the end this thesis is partly about presenting the results of
\ncite{F20} and similar papers.
And partly to build the background theory and compare these results.

\section{What is Reinforcement Learning?}

Reinforcement learning (RL) is a broad topic and a main branch of
\emph{machine learning} alongside \emph{supervised} and \emph{unsupervised
learning}. Because of its broadness it overlaps with other disciplines
such as \emph{control theory} and dynamic programming.

In Reinforcement Learning
we are concerned with finding an optimal policy
for an agent in some environment.
This environment is described by
a sequence of state and action spaces
$\Cal{S}_1, \Cal{A}_1, \Cal{S}_2, \dots$
and rules $P_1, R_1, P_2, \dots$ specifying which states and rewards
and likely to follow after some action is chosen.
One can then specify rules $\pi$, called a \emph{policy},
for how the agent should act in every situation is the environment.
Given an environment and a policy one obtains stochastic process,
that is, a distribution on sequences of states, actions and
rewards.
One can then measure the performance of the policy by looking at
the expected sum of rewards called the \emph{value function}
$V_\pi$ of the policy.
The goal of reinforcement learning is to find an optimal policy $\pi^*$,
maximizing the value function.

$V_\pi$ is viewed as function that evaluates for each \emph{starting state}
$s \in \cl{S}_1$ the expected total rewards when starting in state $s$
and following policy $\pi$.
There might therefore be different optimal policies for each such starting
state.
Traditionally one defines an optimal value function $V^*(s)$
by taking supremum over all policies $\sup_\pi V_\pi(s)$ for every state
$s \in \cl{S}_1$.
Then an optimal policy $\pi^*$ should satisfy $V_{\pi^*} = V^*$,
i.e. it should be optimal uniformly across all starting states $\cl{S}_1$.
The existence of optimal policies defined in this way is a non-trivial
question and we will devote some time on this.

A particular kind class of environments which are called Markov decision processes
(MDPs),
and work with the same state space $\cl{S}$, action space $\cl{A}$ and rules
$P, R$ throughout the process.
They are by far the most well-studied environments.
With an MDP and a value function $V_1$ satisfying certain assumptions 
one can obtain a policy $\pi_1$ by choosing actions
leading to states with high values (according to $V_1$).
Such policies are called \emph{greedy policies}.
We can then evaluate value of $\pi_1$ yielding a new value function $V_2$.
This leads to an operator on the space of value functions called the
\emph{Bellman operator} and should satisfy $TV_1 = V_2$.
This process of applying the Bellman operator
can be continued indefinitely yielding a sequence of value
functions and policies.
Variations of this idea are called \emph{value iteration} and
\emph{policy iteration},
and is derived from dynamic programming.
The relations 
We show that value iteration converges to the optimal value functions
given mild assumptions on the MDP.
Furthermore we show that the optimal value functions is a fixed point
of the Bellman operator: $TV^* = V^*$.
This is called the \emph{Bellman equation} and is the fundamental property
that describes the concept of dynamic programming.

A further categorization of RL-algorithms can be made into
\emph{off-policy} and \emph{on-policy} classes.
This is simply whether the algorithm learns from data
(states, actions and rewards) arising from 
following its own policy (on-policy) or it can learn from more
arbitrary data (off-policy).
This \emph{more arbitrary data} could for example be
the trajectory of another algorithm when interacting with a decision process,
or simply state-action-reward pairs drawn from some distribution.
In this thesis we will put emphasis on off-policy algorithms,
to which both FQI and DQN belong.


\section{What is Q-learning?}

A problem with value functions defined on the set of states $\cl{S}$ is that
picking optimal actions require knowledge of the transition dynamics $P$.
Often we want to design algorithms that do not require such knowledge of $P$,
so called \emph{model-free} algorithms.
To meet this requirement \emph{Q-functions} are introduced, which evaluates
the value of a state-action pair, instead of only a state.

Given a Q-function $Q$, picking best actions according to $Q$ now
merely require maximization over $Q$ itself.
This is obviously an advantage if we are in situations where $P$ is
unknown. %todo find good example
However it turns out also to be more convenient to work with computationally.
In this thesis we show that value and policy iteration can be done
for Q-functions in a virtually identical manner, when the process dynamics
are known.

When the process dynamics are hidden designing algorithms becomes trickier.
In such settings approaches to the problem
fall in two categories. In the \emph{indirect} approaches
one attempts to estimate the process dynamics first and then afterwards
methods for the known-dynamics are applied.
The \emph{direct} approaches basically covers \emph{the rest}.
In the direct category we find the popular \emph{temporal difference}
algorithms on which \emph{fitted Q-iteration} (FQI)
and \emph{deep Q-network} (DQN) as used in \ncite{M15} is based.
Many direct approaching such as FQI and DQN can be seen as
stochastic approximations of the Bellman equation.

\emph{Q-learning} is the category of algorithms that iteratively updates
Q-functions in the attempt to improve the derived policy.
The \emph{deep network} in DQN comes from the concept of (\emph{deep})
\emph{artificial neural networks} (ANNs)
which is a class of function approximators.
The central idea of DQN is then simply to use deep neural networks
to as approximators for the Q-functions.



\section{Overview}


\usetikzlibrary{positioning, shapes}
%\tikzstyle{data}
\tikzset{
  splittwo/.style={
    rectangle split,
    rectangle split parts=2,
    rounded corners, 
    align=center,
    draw=black, very thick,
  },
  nosplit/.style={
    rectangle, 
    rounded corners, 
    align=center,
    draw=black, very thick,
  },
  line/.style={draw, thick, <-},
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}
\begin{tikzpicture}[
    node distance=0.8cm,
  ]
  \node[nosplit] (HDP) {
    History dependent decision process (HDP)
  };
  \node[splittwo] (Schal) [below=0.5cm of HDP] {
    HDP under SchÃ¤ls conditions
    \nodepart{second}
    Existence of optimal policy $\pi^* \in R\Pi$ \\
    and $V^*_n \to V^*$
  };
  \node[splittwo] (QDP) [right=1.4cm of Schal] {
    Finite Q-value uniform HDP (QDP)
    \nodepart{second}
    Convergence of $\wt{Q}_k \to Q^*$.
  };
  \node[nosplit] (MDP) [below=0.5cm of Schal] {
    Markov decision process (MDP)
  };
  \node[splittwo] (Disc) [below=0.5cm of MDP] {
    Discounted action-finite MDP
    \\ under weak continuity assumptions
    \nodepart{second}
    Introduction of Q-functions
    \\ Existence of greedy optimal $\tau^* \in S\Pi$
    \\ $T^k Q \to Q^*$ exponentially in $\gamma$
  };
  \node[splittwo] (Appr) [below=0.5cm of Disc] {
    $\cl{S} = [0,1]^w$
    + strong continuity assumptions
    \nodepart{second} Bound on ANN-approximated Q-functions:
    \\ $\abs{Q^* - \wt{Q}_k} < \ve/(1-\gamma)$
    for every $\ve > 0$
  };
  \node[splittwo] (FQI) [right=1.3cm of Disc] {
    \ncite{F20}:
    $\cl{S} = [0,1]^w$ + various assumptions
    \nodepart{second}
    $\cl{O}(\gamma^{-k} + n^{-\alpha} \log(n)^\beta)$
    asymptotic convergence of
    \\ $Q_{\wt{\tau}_k} \to Q^*$ in $\norm{\cdot}_{1, \mu}$ with FQI
  };
  \node[splittwo] (Async) [right=1.3cm of Appr] {
    \ncite{S97} and \ncite{MH18}:
    Finite MDP + various assumptions
    \nodepart{second}
    $\cl{O}\left( \min\left\{ k^{-\beta/(1-\gamma)},
    \sqrt{k^{-1} \log\log k} \right\} \right)$-asymptotic
    \\ convergence of $\wt{Q}_{k} \to Q^*$
    with asynchronos TD
    \\ And PAC-learnability of synchronos TD
  };
  \draw [->, shorten >=4pt] (HDP) -- (Schal);
  \draw [->, shorten >=4pt] (Schal) -- (MDP);
  \draw [->, shorten >=4pt] (MDP) -- (Disc);
  \draw [->, shorten >=4pt] (Disc) -- (Appr);
  \draw [->, shorten >=4pt] (Schal) -- (QDP);
  \draw [->, shorten >=4pt] (Disc) -- (FQI);
  \draw [->, shorten >=4pt] (Disc) -- (Async);
  \node (Ctop) [above right=0.2cm of HDP] {};
  \node (Cbot) [below right=0.2cm of Appr] {};
  \draw [dashed] (Ctop) -- (Cbot);
  \node (Based) [above=0.4cm of HDP] {Model-based};
  \node (Free) [right=5cm of Based] {Model-free};
\end{tikzpicture}

