
\section{Comparison}
\begin{figure}[H]
\usetikzlibrary{positioning, shapes}
%\tikzstyle{data}
\tikzset{
  splittwo/.style={
    rectangle split,
    rectangle split parts=2,
    rounded corners, 
    align=center,
    draw=black, very thick,
  },
  nosplit/.style={
    rectangle, 
    rounded corners, 
    align=center,
    draw=black, very thick,
  },
  line/.style={draw, thick, <-},
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}
\begin{tikzpicture}[
    node distance=0.8cm,
  ]
  \node[nosplit] (HDP) {
    \defemph{History dependent decision processes}
    \\ (HDPs)
  };
  \node[splittwo] (Schal) [below=0.5cm of HDP] {
    \defemph{HDPs under SchÃ¤ls conditions}:
    \nodepart{second}
    Existence of optimal policy $\pi^* \in R\Pi$ \\
    and convergence of optimal
    \\ value functions $V^*_n \to V^*$
  };
  \node[splittwo] (QDP) [right=1.4cm of Schal] {
    \defemph{Finite HDPs}:
    \\ Under Q-value uniform assumption (QDP)
    \nodepart{second}
    Convergence of $\wt{Q}_k \to Q^*$.
  };
  \node[nosplit] (MDP) [below=0.5cm of Schal] {
    \defemph{Markov decision processes}
  \\ (MDPs)
  };
  \node[splittwo] (Disc) [below=0.5cm of MDP] {
    \defemph{Discounted action-finite MDPs}:
    \\ under weak continuity assumptions
    \nodepart{second}
    Introduction of Q-functions
    \\ Existence of greedy optimal $\tau^* \in S\Pi$
    \\ $T^k Q \to Q^*$ exponentially in $\gamma$
  };
  \node[splittwo] (Appr) [below=0.5cm of Disc] {
    \defemph{Universal approximation by ANNs}:
    \\ $\cl{S} = [0,1]^w$
    + strong continuity assumptions
    \nodepart{second} Bound on ANN-approximated Q-functions:
    \\ $\abs{Q^* - \wt{Q}_k} < \ve/(1-\gamma)$
    for every $\ve > 0$,
    \\ when $k$ and the network is large enough.
  };
  \node[splittwo] (FQI) [right=1.3cm of Disc] {
    \defemph{Bound with the fitted Q-iteration algorithm}:
    \\ \ncite{F20}:
    $\cl{S} = [0,1]^w$ + various assumptions
    \nodepart{second}
    $\cl{O}(\gamma^{-k} + n^{-\alpha} \log(n)^\beta)$
    asymptotic convergence \\ (for some $\alpha, \beta > 0$)
    of $Q_{\wt{\tau}_k} \to Q^*$ in $\norm{\cdot}_{1, \mu}$ with FQI
    \\ for some class of ReLU network approximators
  };
  \node[splittwo] (Async) [right=0.8cm of Appr, yshift=-1cm] {
    \defemph{Bound on finite model-free MDPs}:
    \\ \ncite{S97} and \ncite{MH18}:
    Finite MDP + various assumptions
    \nodepart{second}
    $\cl{O}\left( \min\left\{ k^{-\beta/(1-\gamma)},
    \sqrt{k^{-1} \log\log k} \right\} \right)$-asymptotic
    \\ convergence of $\wt{Q}_{k} \to Q^*$
    with asynchronos TD
    \\ And PAC-learnability of synchronos TD
  };
  \draw [->, shorten >=4pt] (HDP) -- (Schal);
  \draw [->, shorten >=4pt] (Schal) -- (MDP);
  \draw [->, shorten >=4pt] (MDP) -- (Disc);
  \draw [->, shorten >=4pt] (Disc) -- (Appr);
  \draw [->, shorten >=4pt] (Schal) -- (QDP);
  \draw [->, shorten >=4pt] (Disc) -- (FQI);
  \draw [->, shorten >=4pt] (FQI) -- (Async);
  \node (Ctop) [above right=0.2cm of HDP] {};
  \node (Cbot) [below right=0.2cm of Appr] {};
  \draw [dashed] (Ctop) -- (Cbot);
  \node (Based) [above=0.4cm of HDP] {Model-based};
  \node (Free) [right=5cm of Based] {Model-free};
\end{tikzpicture}
\caption{Some of the results presented in this thesis ordered by generality.}
\end{figure}

\section{Conclusion}

In this paper we have build up the theory behind Q-learning,
covering decision models, optimality of policies,
value functions and their iteration methods.
This gave an introduction to Q-learning 
and a general framework from which to understand
and compare results within the field.
We then turned to model-free algorithms 
and presented convergence results for such in a variety
of settings with state space being both finite and infinite and
dynamics being allowed to depend on history or not.
Finally we presented and proved convergence of the fitted Q-iteration
algorithm as obtained in \ncite{F20}.
All together this paints a picture of what Q-learning is,
how it was developed, which topics it is related to,
what its challenges are and what it is possible to
say theoretically about its convergence to optimaliy at present.
Theoretically you could say that Q-learning is solved in many situations,
since, as we have established,
there is convergence guarrantees for broad classes of problems.
However as to how these convergence results relate to practical aspects of
Q-learning we can still say little and as to the succes of the DQN of
\ncite{M15} we are not much further in understanding.
The major reason is that the computational aspects are so important to their 
succes, and this part is mostly ignored in the results we have covered.
Even though we establish results of the related FQI algorithm in \ncite{F20},
it is unclear if it captures the critical aspects of DQN,
such as experience replay.
In \ncite{F20} convergence of FQI is guaranteed given corresponding
increases in iterations, batch size and function space complexity.
It is hard to interpret exactly how large these increases must be
or whether it is practical.

\section{Further directions}

The litterature on Q-learning algorithms and relating topics such as 
function approximation, dynamic programming and artificial neural networks
is vast, and only very little made it into this thesis. 
An obvious direction to go is to review more of the most recent results
in order to give a more complete picture of the field.

\subsubsection{Relation between FQI and DQN}
Find a way to prove or disprove the conjecture in \ncite{F20} that
results about the convergence of FQI can have implications for the 
DQN algorithm.

\subsubsection{Suboptimality of policies}
This is relating to decision processes and value functions.
Through out the paper we discuss a wide array of
approximations of $Q^*$.
The default strategy is then to accept some close-enough approximation $\wt{Q}$
and then pick the greedy policy $\wt{\pi}$ with respect to $\wt{Q}$.
We then measure our deviation from optimality in terms of the distance
$\norm{Q^* - \wt{Q}}_\infty$.
However in most cases we do not estimate the deviation of
$Q_{\wt{\pi}}$ from $Q^*$ which from a theorical point of view should be
a better measure of the sub-optimality of $\wt{\pi}$ compared to $\pi^*$.
Some sources like \ncite{F20} succeed in bounding
$\norm{Q^* - Q_{\wt{\pi}}}_\infty$,
while many others make do with a bound on $\norm{Q^* - \wt{Q}}_\infty$.
To this end it could be interesting to establish relations
between $\norm{Q^* - Q_{\wt{\pi}}}_\infty$ and $\norm{Q^* - \wt{Q}}_\infty$.

\subsubsection{Bernstein polynomials vs. orthogonal projection}
A Bernstein polynomial $B_f$ approximating a function $f$
are constructed by evaluating the
functions at a finite number of points (see \cref{defn:Bfn}).
Since we in this setting are concerned with approximation in the 2-norm,
another approach would be to simply take the orthogonal projection of
$TQ$ onto the span of polynomials of degree less than $n$.
One should keep in mind that this requires integration of
$\abs{TQ(\cdot, a) f_i}$ for every basis polynomial $f_i$,
which is potentially hard to compute.
On the other hand, as the orthogonal projection is distance minimizing,
it should provide the best approximation with polynomials.
The relation between the performances of the Berstein polynomial
and the orthogonal projection, both in terms of accuracy and 
computational complexity, could be interesting analyse.

\section{Notes on references}
The proofs on basic measure theory are inspired by ones found in
\mcite{RH14} and \mcite{K02}.
A good survey on results on optimal policy existence in the special case
of Markov decision processes can be found in \mcite{F12},
however proofs in this source is either missing or sketched
(as one must expect in a survey).

\section{Credits}
I would like to thank PhD-student Jonas Rysgaard Jensen for helping me
out with a proof on the Ionescu Tulcea kernel,
my cousin Rune Harder Bak for reading the mess I've made,
my dormmates at the P.C.Petersens dorm for good company and very necessary
recreational breaks from writing,
my aunt Susanne for letting me stay at her house during the covid-19 and
my the rest of my family for love and support.

