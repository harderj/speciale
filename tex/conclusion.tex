
\section{Comparison}
\begin{figure}[H]
\usetikzlibrary{positioning, shapes}
%\tikzstyle{data}
\tikzset{
  splittwo/.style={
    rectangle split,
    rectangle split parts=2,
    rounded corners, 
    align=center,
    draw=black, very thick,
  },
  nosplit/.style={
    rectangle, 
    rounded corners, 
    align=center,
    draw=black, very thick,
  }
}
\begin{tikzpicture}[
    node distance=0.8cm,
  ]
  \node[splittwo] (HDP) at (0,0) {
    \defemph{HDPs under Sch√§ls conditions}:
    \nodepart{second}
    Existence of optimal policy $\pi^* \in R\Pi$ \\
    and convergence of optimal
    \\ value functions $V^*_n \to V^*$
  };
  \node[splittwo] (QDP) at (8,0) {
    \defemph{Finite HDPs}:
    \\ Under Q-value uniform assumption (QDP)
    \nodepart{second}
    Convergence of $\wt{Q}_k \to Q^*$.
  };
  \node[splittwo] (MDP) at (0,-3.6) {
    \defemph{Discounted action-finite MDPs}:
    \\ under weak continuity assumptions
    \nodepart{second}
    Introduction of Q-functions
    \\ Existence of greedy optimal $\tau^* \in S\Pi$
    \\ $T^k Q \to Q^*$ exponentially in $\gamma$
  };
  \node[splittwo] (ANN) at (0, -7) {
    \defemph{Universal approximation by ANNs}:
    \\ $\cl{S} = [0,1]^w$
    + strong continuity assumptions
    \nodepart{second} Bound on $\cl{RN}$-approximated Q-functions:
    \\ $\abs{Q^* - \wt{Q}_k} < \ve/(1-\gamma)$
    for every $\ve > 0$,
    \\ when $k$ and the network is large enough.
  };
  \node[splittwo] (FQI) at (8, -3.6) {
    \defemph{Bound with the fitted Q-iteration algorithm}:
    \\ \ncite{F20}:
    $\cl{S} \subseteq \R^w$ compact + complicated assumptions
    \nodepart{second}
    $\cl{O}(\gamma^{-k} + n^{-\alpha} \log(n)^\beta)$
    asymptotic convergence \\ (for some $\alpha, \beta > 0$)
    of $Q_{\wt{\tau}_k} \to Q^*$ in $\norm{\cdot}_{1, \mu}$ with DQI
    \\ with large enough sparse ReLU network structure
  };
  \node[splittwo] (Async) at (8,-7){
    \defemph{Bound on finite MDPs}:
    \\ \ncite{S97} and \ncite{MH18}:
    Finite MDP + various assumptions
    \nodepart{second}
    $\cl{O}\left( \min\left\{ k^{-\beta/(1-\gamma)},
    \sqrt{k^{-1} \log\log k} \right\} \right)$-asymptotic
    \\ convergence of $\wt{Q}_{k} \to Q^*$
    with asynchronos TD
  };
  \draw [-latex] (HDP) -- (QDP);
  \draw [-latex] (HDP) -- (MDP);
  \draw [-latex] (MDP) -- (ANN);
  \draw [-latex] (MDP) -- (Async);
  \draw [-latex] (MDP) -- (FQI);
  \node (uppermiddle) at (3.7,2) {};
  \node (lowermiddle) at (3.7,-9) {};
  \node (middleleft) at (-3, -1.3) {};
  \node (middleright) at (12, -1.3) {};
  \draw [dashed] (uppermiddle) -- (lowermiddle);
  \draw [dashed] (middleleft) -- (middleright);
  \node at (0, 1.5) {Model-dependent HDP};
  \node at (8, 1.5) {Model-free HDP};
  \node at (0, -1.7) {Model-dependent MDP};
  \node at (8, -1.7) {Model-free MDP};
\end{tikzpicture}
\caption{Some of the convergence bound presented in this thesis.
Arrows $A \to B$ implies that setting $B$ is a special
case of setting $A$.}
\end{figure}

Overall every result provide insights to some classes of decision processes
which is not covered by any of the other.
For the history dependent processes we have convergence guarantees
in the finite Q-value uniform (QDP) setting, as proved in
\ncite{MH18}. This specializes to finite model-free MDPs, but presents no
new insight in this setting, as such a result was already covered
in \ncite{WD92}. It does however provide evidence that results on Q-learning
may be extended to a history dependent setting.

Turning to MDPs,
as we have discussed, the exponential convergence of the model-dependent
Q-iteration while impressive is often not practical and so should be
seen more as a theoretical background for the other results.
The model-dependent result using ReLU network approximated Q-functions
in the continuous setting
shows that model-dependent methods, when applicable, provide the same
exponential convergence to a near-optimal approximated Q-function.
This is comparable to the convergence result by \ncite{F20}, except that it is
less complicated and requires less assumptions except for the critical
assumption of model-dependency.
However because of it being model dependent, it not clear how much
information about the model-free setting it implies.

The result by \ncite{F20} of DQI has the advantage of being model-free
and working with a continuous state space, unlike the other results.
Another advantage is that it succeeds in evaluating the distance to the
actual policy evaluation $Q_{\wt{\tau}}$ which provides better evidence
of the nearness to optimality of its output policy, compared to all the
other results, which only measures the distance to the approximating
Q-functions.
A weak point is however that it measures distance in the $\norm{\cdot}_{1, \mu}$
norm, which is a bit non-standard and weaker than the rest of the results,
which measures distance in $\norm{\cdot}_\infty$.
Another weak point is that the result and its assumptions are
quite complicated and it is not clear how big the it includes
are.

Specializing to finite model-free MDPs we get much better convergence rates
in \ncite{S97} even using the $\norm{\cdot}_\infty$ norm.
This is however at the expense requiring finiteness and
a full table of state-action values instead of an approximating scheme.
Also distance is measured between $Q^*$ and $\wt{Q}_K$ rather than
$\wt{Q}_{\wt{\pi}_K}$ as in \ncite{F20}.

\section{Conclusion}

In this thesis we have build up the theory behind Q-learning,
covering decision models, optimality of policies,
value functions and their iteration methods.
This gave an introduction to Q-learning 
and a general framework from which to understand
and compare results within the field.
We then turned to model-free algorithms 
and presented convergence results for such in a variety
of settings with state space being both finite and infinite and
dynamics being allowed to depend on history or not.
Finally we presented and proved convergence of the fitted Q-iteration
algorithm as obtained in \ncite{F20}.
All together this paints a picture of what Q-learning is,
how it was developed, which topics it is related to,
what its challenges are and what it is possible to
say theoretically about its convergence to optimaliy at present.
Theoretically you could say that Q-learning is solved in many situations,
since, as we have established,
there is convergence guarrantees for broad classes of problems.
However as to how these convergence results relate to practical aspects of
Q-learning we can still say little and as to the success of the DQN of
\ncite{M15} we are not much further in understanding.
The major reason is that the computational aspects are so important to their 
success, and this part is mostly ignored in the results we have covered.
Even though we establish results of the related FQI algorithm in \ncite{F20},
it is unclear if it captures the critical aspects of DQN,
such as experience replay.
In \ncite{F20} convergence of FQI is guaranteed given corresponding
increases in iterations, batch size and function space complexity.
It is hard to interpret exactly how large these increases must be
or whether it is practical.
However it is also possible that DQN can be seen largely as an instance of FQI 
and that the results we have covered from \ncite{F20} does explain
part of its success.

\section{Further directions}
\subsubsection{Examples}
In this thesis we talk little about concrete decision processes, only
applying our theory on the rather trivial gridworld environment in
\cref{ex:gridworld}.
Examples are important drivers much of both theoretical and more empirical
research in RL.
Applying the theory in this thesis on concrete cases would be a interesting
next step.

\subsubsection{Relation between FQI and DQN}
Find a way to prove or disprove the conjecture in \ncite{F20} that
convergence bounds on the FQI algorithm can have implications for the 
DQN algorithm.
Answering this question probably requires research beyond what can be
found in litterature already, as otherwise \ncite{F20} would have
drawn on such sources.

\subsubsection{Application of the results of \mcite{MR07}}
Though \ncite{MR07} does not make explicit bounds for the distance of an approximating
Q-function to $Q^*$,
its results provide strong theoretical results
of model-free Q-learning given a finite set of basis functions. 
Such a finite set of basis functions is provided by e.g.
polynomials. It could be interesting to combine the results of \ncite{MR07}
with various results in approximation theory such as theory on Bernstein
polynomials and compare the resulting bounds of convergence to the bounds in
\ncite{F20}.

\subsubsection{Suboptimality of policies}
This is relating to decision processes and value functions.
Through out this thesis we discuss a wide array of
approximations of $Q^*$.
The default strategy is then to accept some close-enough approximation $\wt{Q}$
and then pick the greedy policy $\wt{\pi}$ with respect to $\wt{Q}$.
We then measure our deviation from optimality in terms of the distance
$\norm{Q^* - \wt{Q}}_\infty$.
However in most cases we do not estimate the deviation of
$Q_{\wt{\pi}}$ from $Q^*$ which from a theorical point of view should be
a better measure of the sub-optimality of $\wt{\pi}$ compared to $\pi^*$.
Some sources like \ncite{F20} succeed in bounding
$\norm{Q^* - Q_{\wt{\pi}}}_\infty$,
while most others is satisfied with a bound on $\norm{Q^* - \wt{Q}}_\infty$.
To this end it could be interesting to establish relations
between $\norm{Q^* - Q_{\wt{\pi}}}_\infty$ and $\norm{Q^* - \wt{Q}}_\infty$.

\subsubsection{Bernstein polynomials vs. orthogonal projection}
A Bernstein polynomial $B_f$ approximating a function $f$
are constructed by evaluating the
functions at a finite number of points (see \cref{defn:Bfn}).
Since we in this setting are concerned with approximation in the 2-norm,
another approach would be to simply take the orthogonal projection of
$TQ$ onto the span of polynomials of degree less than $n$.
One should keep in mind that this requires integration of
$\abs{TQ(\cdot, a) f_i}$ for every basis polynomial $f_i$,
which is potentially hard to compute.
On the other hand, as the orthogonal projection is distance minimizing,
it should provide the best approximation with polynomials.
The relation between the performances of the Berstein polynomial
and the orthogonal projection, both in terms of accuracy and 
computational complexity, could be interesting analyse.

\section{Notes on references}
The proofs on basic measure theory are inspired by ones found in
\mcite{RH14} and \mcite{K02}.
A good survey on results on optimal policy existence in the special case
of Markov decision processes can be found in \mcite{F12},
however proofs in this source is either missing or sketched
(as one must expect in a survey).

\section{Credits}
I would like to thank PhD-student Jonas Rysgaard Jensen for helping me
out with a proof on the Ionescu Tulcea kernel,
my cousin Rune Harder Bak for reading the mess I've made,
my dormmates at the P.C.Petersens dorm for good company and very necessary
recreational breaks from writing,
my aunt Susanne for letting me stay at her house during the covid-19 and
my family for love and support.

