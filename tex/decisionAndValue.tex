
\subsection{General decision model}
In the quest to have a united framework to talk about results from
several different models we define here a quite general model.
One which is quite close to in generality can be found in
[ref. to Schal].
In this section recall that $\ul{\R} = \R \cup \{-\infty\}$,
$\ol{\R} = \R \cup \{\infty\}$ and
$\ol{\ul{\R}} = \R \cup \{\pm \infty\}$.

\begin{defn}[Decision model]
  A general \defemph{decision} model is determined by
  \begin{enumerate}
    \item $(\Cal{S}_n, \Sigma_{\Cal{S}_n})_{n \in \N}$ a 
      measurable space of \defemph{states} for each timestep.
    \item $(\Cal{A}_n, \Sigma_{\Cal{A}_n})_{n \in \N}$ a 
      measurable space of \defemph{actions} for each timestep.
  \end{enumerate}
  for each $n \in \N$ we define the so called \defemph{history} spaces
  \[ \Cal{H}_n = \Cal{S}_1 \times \Cal{A}_1
    \times \Cal{S}_2 \times \ol{\ul{\R}} \times \Cal{A}_2
    \times \Cal{S}_3 \times \ol{\ul{\R}} \dots \times \Cal{S}_n,
    \Cal{H}_\infty = \Cal{S}_1 \times \Cal{A}_1 \times \Cal{S}_2 \times
    \ol{\ul{\R}} \times \dots
  \]
  with associated product $\sigma$-algebras
  \begin{enumerate} \setcounter{enumi}{2}
    \item $(P_n)_{n \in \N}$ a sequence of
      $\Cal{H}_n \times \Cal{A}_n \leadsto \Cal{S}_{n+1}$ kernels
      called the \defemph{transition} kernels.
    \item $(R_n)_{n \in \N}$ a sequence of
      $\Cal{H}_{n+1} \leadsto \ol{\ul{\R}}$ kernels
      called the \defemph{reward} kernels.
  \end{enumerate}
\end{defn}
Some authors refers to this as a \emph{dynamic progamming} model.
Notice the slight irregularity in the beginning of the history spaces:
We are missing a reward state after $\Cal{S}_1$. We could avoid
this by introducing some start reward, but we will be careless.

The vast majority of sources considered in this paper actually specialize
the DP model with the following:
\begin{asm}[One state and action space]
  $\Cal{S}_1 = \Cal{S}_2 = \dots \defeq \Cal{S}$
  $\Cal{A}_1 = \Cal{A}_2 = \dots \defeq \Cal{A}$
  \label{asm:oneStateActionSpace}
\end{asm}
However we will do without this for the rest of this section in order to
present some results in the generality they deserve.
One could ask if it is possible to embed the general DP model into one
with \cref{asm:oneStateActionSpace} by setting
$\Cal{S} \defeq \bigcup_{i\in\N} \Cal{S}_i$ and
$\Cal{A} \defeq \bigcup_{i\in\N} \Cal{A}_i$ or similar.
One attempt at this can be found in [BS SOC, chp. 10],
but this will not be covered here. %consider covering it

For a DP model we can define
\begin{defn}[Policy]
  A (randomized) \defemph{policy} $\pi = (\pi_n)_{n \in \N}$
  is a sequence of $\Cal{H}_n \leadsto \Cal{A}_n$ kernels.
  The set of all policies we denote $R\Pi$.
  The policy $\pi$ is called \defemph{semi Markov} if each $\pi_i$ only depends
  on the first and last state in the history
  and is called \defemph{Markov} if only the last.
  The sets are denoted $sM\Pi$ and $M\Pi$.
 SFurthermore $\pi$ is called \defemph{deterministic} if all $\pi_i$
  are degenerate, i.e. are actually measurable functions from
  $\Cal{H}_n$ to $\Cal{A}_n$. 
  Under \cref{asm:oneStateActionSpace}
  it makes sense to make a (Markov) policy $(\pi, \pi, \dots)$,
  where $\pi$ only depends on the last state.
  Such a policy is called \defemph{stationary},
  and the set of them denoted $S\Pi$.
\end{defn}
We have the following inclusions
\begin{align*}
  S\Pi \subseteq M\Pi &\subseteq sM\Pi \subseteq R\Pi
  \\ DS\Pi \subseteq DM\Pi &\subseteq DsM\Pi \subseteq D\Pi
\end{align*}

\begin{prop}
A dynamic progamming model together with a policy $\pi$ defines a
probability kernel $\kappa_\pi : \Cal{S}_1 \to \Cal{H}_\infty$.
\end{prop}
\begin{proof}
  This is the Ionescu-Tulcea kernel generated by
  $\dots R_2 P_2 \pi_2 R_1 P_1 \pi_1$.
\end{proof}
This kernel yields a probability measure $\kappa_\pi \mu$ on $\Cal{H}_\infty$
for every $\mu \in \Cal{S}_1$. In particular for any $s \in \Cal{S}_1$
$\kappa_\pi \delta_s$ yields the measure $\kappa(\cdot \mid s)$
and we shall occasionally write this $\kappa_\pi s$ and
integration with respect to it $\E^\pi_s$.

Across litterature generally %todo: backup this claim
any function mapping a state space $\Cal{S}$ to $\ol{\ul{\R}}$
can be called a (state)
\defemph{value} function. Similarly any $\ul{\ol{\R}}$ valued function
on pairs of states and actions can be called (state)
\defemph{action value} or \defemph{Q}- function.
The idea behind such functions are commonly to estimate the
cumulative rewards associated with a state or state-action pair
and the trajectory of states it can lead to.
In order to define some of the most standard of value functions,
which we call \defemph{ideal} to avoid confusion, we will need one of
the following conditions:

\begin{cond}{$F^+$}
  $R_i(\{\infty\} \mid h) = 0$ for all
  $h \in \Cal{H}_{i+1}$ and $i \in \N$
  \label{cond:F+}
\end{cond}
\begin{cond}{$F^-$}
  $R_i(\{-\infty\} \mid h) = 0$ for all
  $h \in \Cal{H}_{i+1}$ and $i \in \N$
  \label{cond:F-}
\end{cond}
When assuming either of (\cref{cond:F+}) or (\cref{cond:F-})
adding rewards cannot lead to a $\infty - \infty$ situation,
and the following definition makes sense 
\begin{defn}
  Let $\Cal{R}_i : \Cal{H}_\infty \to \ol{\ul{\R}}$ be the projection onto the
  $i$th reward. Define
  \[ V_{n,\pi}(s) = \E_s^\pi \sum_{i=1}^n \Cal{R}_i \]
  called the $k$th finite \defemph{ideal} value function.
  When $n=0$ $\forall \pi : V_{0,\pi} = V_0 \defeq 0$.
\end{defn}
These are also called \emph{finite horizon} value functions.

We would like to extend this to an infinite horizon value function,
i.e. letting $n$ tend to $\infty$. To ensure that the integral is well-defined
we need one of the following conditions
\begin{cond}{P} $R_i([0,\infty] \mid h) = 1$,
  $\forall h \in \Cal{H}_{i+1}, i \in \N$
  \label{cond:P}
\end{cond}
\begin{cond}{N} $R_i([-\infty, 0] \mid h) = 1$
  $\forall h \in \Cal{H}_{i+1}, i \in \N$
  \label{cond:N} 
\end{cond}
\begin{cond}{D} There exist a bound $R_{\max} > 0$ and a
  $\gamma \in [0,1)$ called the \defemph{discount} factor such that
  $R_i([-R_{\max} \gamma^i, R_{\max} \gamma^i]) = 1$
  $\forall h \in \Cal{H}_{i+1}, i \in \N$
  \label{cond:D}
\end{cond}

\begin{defn}
  Assume (P), (N) or (D). We define the (infinite horizon) ideal value
  function by
  \[ V_\pi(s) = \E_s^\pi \lim_{n \to \infty} \sum_{i=1}^n \Cal{R}_i \]
\end{defn}

\begin{prop}
  The ideal value functions $V_{n,\pi}, V_\pi$ are measurable
  into $(\ul{\ol{\R}}, \ul{\ol{\bb{B}}})$.
\end{prop}
\begin{proof}  
  Use \cref{prop:intKerMeas}.
\end{proof}

\begin{prop}
  Under \cref{cond:P}, \cref{cond:N} or \cref{cond:D} we have
  $\lim_{n\to\infty} V_{n, \pi} = V_\pi $
  for all $\pi \in R\Pi$.
\end{prop}
\begin{proof}
  By monotone or dominated convergence.
\end{proof}

\subsubsection{Optimal policies}

Let $(\Cal{S}_n, \Cal{A}_n, P_n, R_n)_{n \in \N}$ be a DP model.

\begin{asm}(Reward independence)
  $P_n, R_n$ and policies are only allowed to depend on the
  states and actions.
  \label{asm:rewardIndep}
\end{asm}

In all sources known to this writer \cref{asm:rewardIndep} is assumed.
This is a bit of a puzzle since it is obvious that one could
want to define algorithms (policies) that take into account which rewards
they received in the past.
We will also do this but stick to the standard and 
never attempt to evaluate ideal value functions of
policies that depend on rewards.
Thus we will assume \cref{asm:rewardIndep} henceforth with including the
shrinkage of the set of general policies $R\Pi$ that it entails.

A neat consequence of \cref{asm:rewardIndep} when talking about
value functions is that we can reduce the reward kernels to functions
$r_i : \Cal{H}_{i+1} \to \ul{\R} = h \to \int r \difd R_i(r \mid h)$
which are measurable (due to \cref{prop:intKerMeas}).

\begin{defn}[Optimal value functions] 
  \begin{align*}
    V_n^*(s) \defeq & \; \sup_{\pi \in R\Pi} V_n^\pi(s) &
    V^*(s) \defeq & \; \sup_{\pi \in R\Pi} V^\pi(s)
  \end{align*}
  are called the \defemph{optimal} value functions.
  A policy $\pi^* \in R\Pi$ for which $V_{\pi^*} = V^*$ is called an
  \defemph{optimal} policy.
  If $V_{n, \pi^*} = V^*_n$ it is called $n$-optimal.
\end{defn}

An interesting fact about the optimal value functions is that they
might not be Borel measurable [todo ref to counterexample]
even in the finite case.
After all we are taking a supremum over
sets of policies which have cardinality of at least the continuum.
However it is sometimes possible to show that they are
universally measurable, thus Lebesgue measurable and therefore
standard Lebesgue integration is possible.
We will take these discussions as they occur in various settings.

At this point many interesting questions can be asked.
\begin{enumerate}
  \item To which extend does an optimal policy $\pi^*$ exist?
  \item Does $V_n^*$ converge to $V^*$?
  \item In case there is some sort of optimal policy
    in which classes of policies has a representative?
\end{enumerate}
These questions has been answered in a variety of settings.
We will address these question in order by strength of assumptions
they require as far as this is possible.

In a quite general setting, questions 1 and 2
was investigated by M. Sch채l in 1974
[todo ref. to On Dynamic Programming:
Compactness of the space of policies, 1974].
Here some additional structure on our model is imposed:
\begin{sett}[Sch채l]
  \begin{enumerate}
    \item $V_\pi < \infty$ for all policies $\pi \in R\Pi$.
    \item $(\Cal{S}_n, \Sigma_{\Cal{S}_n})$ is assumed to be standard Borel.
      I.e. $\Cal{S}_n$ is a non-empty Borel subset of a Polish space
      and $\Sigma_{\Cal{S}_n}$ is the Borel subsets of $S_n$.
    \item $(\Cal{A}_n, \Sigma_{\Cal{A}_n})$ is similarly assumed to be
      standard Borel.
    \item $\Cal{A}_n$ is compact.
    \item $\forall s \in \Cal{S}_1 :
      Z_n = \sup_{N \geq n} \sup_{\pi \in R\Pi} \sum_{t=n+1}^N
      \E_s^\pi r_n \to 0$ as $n \to \infty$.
  \end{enumerate}
  \label{sett:Schal}
\end{sett}

In this setting Sch채l introduced two set of criteria for the existence
of an optimal policy:

\begin{cond}{S}
  \begin{enumerate}
    \item The function \[
	(a_1, a_2, \dots, a_n) \mapsto
	P_n(\cdot \mid s_1, a_1, s_2, a_2, \dots, s_n, a_n)
      \]
      is set-wise continuous (hence the name \defemph{S})
      for all $s_1, \dots, s_n \in \Cal{S}^{\ul{n}}$.
    \item $r_n$ is upper semi-continuous.
  \end{enumerate}
  \label{cond:S}
\end{cond}

\begin{cond}{W}
  \begin{enumerate}
    \item The function
      \[(h_n, a_n) \mapsto P_n(\cdot \mid h_n, a_n)\]
	is weakly continuous (hence the name \defemph{W}).
    \item $r_n$ is continuous.
  \end{enumerate}
  \label{cond:W}
\end{cond}

\begin{thm}[Existence and convergence of optimal policies in DP]
  When either \cref{cond:S} or \cref{cond:W} hold then
  \begin{enumerate}
    \item There exist an optimal policy $\pi^* \in R\Pi$.
    \item $V^*_n \to V^*$ as $n \to \infty$.
  \end{enumerate}
\end{thm}
\begin{proof}
  We refer to [todo ref: On Dynamic Programming: Compactness of the space of
  policies, M. Sch채l 1974]. %todo: or do we?
\end{proof}

\subsection{Markov Decisions Process}

\begin{sett}[Markov Decision Process]
  \begin{itemize}
    \item \Cref{asm:oneStateActionSpace} i.e. there is only one state
      and action space $\Cal{S}$, $\Cal{A}$.
    \item $P_n$ depends only on $s_n$ and $a_n$ and does not
      differ with $n$. I.e. there exists a kernel $P$ such that
      $P_n(\cdot \mid s_1, \dots, s_n, a_n) = P(\cdot \mid s_n, a_n)$
      for all $n \in \N$. We will write $P$ instead of $P_n$ understanding
      kernel compositions as if using $P_n$.
    \item $r_n$ depends only on $s_n$ and $a_n$ and does not differ
      with $n$ except for a potential discount.
      I.e. there exists a measurable function
      $r:\Cal{S}\times\Cal{A} \to \ol{\ul{\R}}$
      such that
      $r = r_n/\gamma^{n-1}$ for all $n \in \N$ (in the case where we 
      are not discounting set $\gamma = 1$).
  \end{itemize}
  \label{sett:MDP}
\end{sett}
%

\subsection{Bertsekas-Shreve framework}
The theory described here is largely based on
[ref to Bertsekas-Shreve, Stochastic Optimal Control].
Their framework is cost-based as opposed to the this paper reward-based outset.
This means that positive and negative, upper and lower, supremum and infimum,
ect. are opposite to the source.
\begin{sett}[BS]
  \begin{itemize}
    \item We consider an MDP (see \cref{sett:MDP}).
    \item $\Cal{S}$ and $\Cal{A}$ are Borel spaces.
    \item $\Cal{A}$ is compact.
    \item $P(S \mid \cdot)$ is continuous for any $S \in \Sigma_{\Cal{S}}$.
    \item $r(s,a) = \gamma^{1-i} \int x \difd R(x \mid s, a)$ 
      is upper semicontinuous and uniformly bounded from above
      (least upper bound denoted $0 < R_{\max} < \infty$).
    \item The policies must consist of
      universally measurable probability kernels.
  \end{itemize}
  \label{sett:BS}
\end{sett}
The original setup in [ref to Bertsekas-Shreve, Stochastic Optimal Control]
is slightly different than the setup here presented.
Besides having a state and action space, it also features a 
non-empty Borel space called the
\emph{disturbance space} $W$, a \emph{disturbance kernel}
$p: \Cal{S} \times \Cal{A} \to W$,
instead of a transition kernel which on the other hand is a deterministic
\emph{system function} $f : \Cal{S} \times \Cal{A} \times W \to \Cal{S}$
which should be Borel measurable.
Moreover it allows for constrains on the action space for each state.
This is made precise by a function $U:\Cal{S} \to \Sigma_{\Cal{A}}$
and a restriction on $R\Pi$ that all policies $\pi$ should satisfy
$\pi(U(s) \mid s) = 1$.
Lastly the rewards are interpreted as negative costs, and thus
$g$ is required to be semi \emph{lower}continuous.

By setting $P(\cdot \mid s, a) = f(s, a, p(\cdot \mid s, a))$
and maximizing rewards of upper semicontinuous instead of
minimizing lower semicontinuous ones, we fully capture
all aspects of the original model and its results,
except the for the action constrains. %todo make a more precise argument

Notice that \cref{sett:BS} implies (\cref{cond:F+}).
Throughout this section (\cref{cond:P}), (\cref{cond:N}) or (\cref{cond:D})
are always assumed. Some results only hold for some of these conditions
and we will indicate this by e.g. (\cref{cond:D}) (\cref{cond:P}) when
the result only holds for the discounted and positive case.
At this point it makes sense to define

\begin{defn}[The $T$-operators]
  For a stationary policy $\pi$ and measurable $V:\Cal{S} \to \ol{\ul{\R}}$
  with $V \geq 0$, $V \leq 0$ or $\abs{V} < \infty$
  we define the operators 
  \[ P_\pi V \defeq s \mapsto \int V(s') \difd P\pi(s' \mid s) \]
  \[ T_\pi V \defeq s \mapsto \int r(s, a)
  + \gamma V(s') \difd (P \pi)(a, s'\mid s) \]
  \[ T V \defeq s \mapsto \sup_{a \in \Cal{A}} T_a V(s) \]
  where $T_a = T_{\delta_a}$.
\end{defn}

\begin{prop}
  The operators $P_\pi, T_\pi$ and $T$ commutes with limits.
\end{prop}
\begin{proof}
  By monotone or dominated convergence theorems.
\end{proof}

\begin{prop} Let $\pi = (\pi_1, \pi_2, \dots)$ be a Markov policy. Then
  $V_{k, \pi} = T_{\pi_1} \dots T_{\pi_k} V_0$ and
  $V_\pi = \lim_{k \to \infty} T_{\pi_1} \dots T_{\pi_k} V_0$.
  \label{prop:VpiTLimit}
\end{prop}
\begin{proof}
  todo
\end{proof}

\begin{prop} Let $\pi$ be a stationary policy then
  $T_\pi V_\pi = V_\pi$.
  \label{prop:VpiFP}
\end{prop}
\begin{proof}
  By \cref{prop:VpiTLimit}
  $T_\pi V_\pi = T_\pi \lim_{k \to\infty} T_{\pi}^k V_0
  = \lim_{k \to\infty} T_\pi^{k+1} V_0 = V_\pi$.
\end{proof}

\begin{prop}
  Under (\cref{cond:D}) for any $\pi \in R\Pi$ we have
  \[ \abs{V_{n,\pi}}, \abs{V_\pi}, \abs{V^*_k}, \abs{V^*}
  \leq V_{\max} \defeq R_{\max}/(1-\gamma) \]
  \label{prop:Vbounded}
\end{prop}
\begin{proof}
  For any $\pi \in R\Pi$
  \[ \abs{V_\pi(s)} \leq \E_s^\pi \sum_{i \in \N} \abs{r_i}
    \leq \sum_{i \in \N} \gamma^{i-1} R_{\max}
  = R_{\max} / (1-\gamma) \]
  This also covers $V_{n, \pi}$.
\end{proof}

\begin{prop} (\cref{cond:D})

  $T$ and $T_\pi$ are $\gamma$-contractive on $\Cal{L}_\infty(\Cal{S})$.
  \label{prop:TpiContracts}
\end{prop}
\begin{proof}
  Let $V, V' \in \Cal{L}_\infty(\Cal{S})$
  and let $K = \norm{V - V'}_\infty$.
  Then
  \[ \abs{T^\pi V - T^\pi V'}
    = \gamma \abs{\int V(s') - V'(s') \difd P\pi(s' \mid s)}
  \leq \gamma K \]
  For $T$ use that same argument and the fact that
  $\abs{\sup_x f(x) - \sup_{y} g(y)} \leq
  \abs{\sup_x f(x) - g(x)}$ for any $f,g : X \to \ul{\R}$.
\end{proof}

\begin{cor} (\cref{cond:D})

  Let $\pi \in S\Pi$ be a stationary policy.
  $V_\pi$ is the unique bounded fixed point of $T_\pi$
  in $\Cal{L}_\infty(\Cal{S})$.
\end{cor}
\begin{proof}
  By \cref{prop:VpiFP} $V_\pi$ is a fixed point.
  By \cref{prop:TpiContracts} and Banach fixed point theorem we get uniqueness.
\end{proof}

%proposition 8.6 Stoch. Opt. Control
\begin{prop}[Prop. 8.6 in BS]
  $V^*_k = T^k V_0$ and is upper semicontinuous.
  Furthermore for any $k \in \N$
  there exists a deterministic, Markov, Borel-measurable $k$-optimal policy
  $\pi^*_k = (\pi^*_{k,1}, \pi^*_{k,2}, \dots, \pi^*_{k,k}, \dots) \in DM\Pi$.
  These policies satisfy for any $i < k$
  $\pi^*_{i} = (\pi^*_{k,k-i}, \dots, \pi^*_{k,k}, \dots)$.
\end{prop}


%cor. 9.17.2
\begin{thm}[Cor. 9.17.2 in BS]
  Under (\cref{cond:N}) or (\cref{cond:D})
  $V^* = \lim_{k\to\infty} V_k^*$ and is upper semicontinuous.
  Furthermore there exist a deterministic
  stationary, Borel-measurable policy $\pi^*$.
  \label{thm:BScor9.17.2}
\end{thm}

\begin{prop}
  $V^* = T_{\pi^*} V^* = T V^*$
  
  (D) $V^*$ is the unique fixed point of $T$ in $\Cal{L}_\infty(\Cal{S})$.
\end{prop}
\begin{proof}
  Since $\pi^*$ is optimal $V^* = V_{\pi^*} = T_{\pi^*} V_{\pi^*}$ by
  \cref{prop:VpiFP}.
  By \cref{thm:BScor9.17.2} $T V^* = T \lim_{k\to\infty} T^k V_0 =
  \lim_{k\to\infty} T^{k+1} V_0 = V^*$.
  If (D) holds $V^* \in \Cal{L}_\infty(\Cal{S})$ so \cref{prop:TpiContracts}
  and Banach fixed point theorem ensures uniqueness.
\end{proof}

\subsubsection{Analytic setting}

\begin{sett}[BS Analytic]
  The same as \cref{sett:BS} except:
  $P$ is not necessarily continuous.
  $r$ is upper semianalytic.
  $\Cal{A}$ is not necessarily compact, but
  there exists a $k \in \N$ such that
  $\forall \lambda \in \R, n \geq k, s \in \Cal{S}$
  \[ A^\lambda_n(s) = \left\{ a \in \Cal{A} \Mid r(s, a)
  + \gamma \int V^*_n P(\cdot \mid s, a) \geq \lambda \right\} \]
  is a compact subset of $\Cal{A}$.
  \label{sett:BSA}
\end{sett}

\begin{thm}[Prop. 9.17 BS]
  Under \cref{sett:BSA} we have
  $V^* = \lim_{n \to \infty} V^*_n$ for all $s \in \Cal{S}$
  and there exists a optimal policy $\pi^*$ which is stationary
  and deterministic.
\end{thm}
\begin{proof}
  We refer to [todo ref to Bertsekas and Schreve, Stochastic Optimal Control:
  The Discrete-Time Case, prop. 9.17].
\end{proof}

\subsection{Q-functions}

The letter Q originates to a PhD thesis by C. Watkins from 1989
[todo ref C. Watkins, 1989]. Upon his definition he noted
\begin{displayquote}
  ``This is much simpler to calculate than [$V_\pi$]
  for to calculate [$Q_\pi$] it is only necessary to look one
  step ahead [\ldots]''
\end{displayquote}
A clear advantage of working with Q-function
$Q:\Cal{S}\times\Cal{A} \to \Rext$ rather than a value function
$V:\Cal{S}\to \Rext$,
is that finding the optimal action in state $s$
requires only a maximization over the Q-function itself:
$a = \argmax_{a \in \Cal{A}} Q(s,a)$.
This should be seen in contrast to finding a best action according to $V$:
$a = \argmax_{a \in \Cal{A}} r(s,a) + \gamma \E_{P(\cdot \mid s,a)} V$.
This requires taking an expectation with respect to the transition kernel
$P$. Later we will study settings where we are not allowed to know
the transition kernel when attempting to find the optimal strategy.
In these situations the advantage of Q-functions is clear.
For now however the transition kernel will remain known and we
will in this section see how the results of state-value functions
translate to Q-functions.
The results in this section is original in the generality here presented,
as I was unable to find them elsewhere.

\begin{defn}
  Let $\pi \in R\Pi$.
  Define
  \[ Q_{k, \pi}(s, a) = r(s, a) + \gamma \E_{P(\cdot \mid s, a)} V_{k, \pi}
  ,\qquad Q_\pi = r(s, a) + \gamma \E_{P(\cdot \mid s, a)} V_\pi \]
\end{defn}
We define $Q_{0,\pi} = r + \gamma \E V_0 = r \defeq Q_0$.


\begin{prop}
  $\lim_{k \to \infty} Q_{k, \pi} = Q_\pi$
  \label{prop:QpiConverges}
\end{prop}
\begin{proof}
  (D) By dominated convergence or (P/N) by monotone convergence theorem.
\end{proof}

\begin{defn}
  \[ Q^*_k = \sup_{\pi \in R\Pi} Q_{k, \pi}
  , \qquad Q^* = \sup_{\pi \in R\Pi} Q_\pi \]
\end{defn}

\begin{defn}[$T$ operators for Q-functions]
  For any stationary policy $\pi \in S\Pi$
  and measurable $Q:\Cal{S} \times \Cal{A} \to \ol{\ul{\R}}$ with
  $Q \geq 0, Q \leq 0$ or $\abs{Q} < \infty$ we define
  \[ P_\pi Q(s, a) = \int Q(s', a') \difd \pi P(s', a' \mid s, a) \]
  \[ T_\pi Q = r + \gamma P_\pi Q \]
  \[ T Q(s, a) = r(s, a) + \gamma
  \int \sup_{a' \in \Cal{A}} Q(s', a') \difd P(\cdot \mid s, a) \]
  where $T_a = T_{\delta_a}$.
\end{defn}

\begin{prop} Let $\pi = (\pi_1, \dots, \pi_k, \dots) \in M\Pi$ then
  \begin{itemize}
    \item $T_{\pi_k} Q_{k-1, \pi}
      = r + \gamma \E T_{\pi_k} V_{k-1, \pi}$
    \item $Q_{k, \pi} = T_{\pi_1} \dots T_{\pi_k} r$.
  \end{itemize}
  \label{prop:QpiVpi}
\end{prop}
\begin{proof} %todo do this more thoroughly
  The first statement is almost by definition.
  For the second use the first inductively.
\end{proof}

\begin{prop}
  $Q^*_k = r + \gamma \E V^*_k$
  and $Q^* = r + \gamma \E V^*$.
  \label{prop:Q*V*}
\end{prop}
\begin{proof}
  $Q^* = \sup_\pi Q_\pi = \sup_\pi (r + \gamma \E V_\pi)
  = r + \gamma \sup_\pi \E V_\pi
  = r + \gamma \E V^*$
  where in the fourth equality we have used that
  $V^* \geq V_\pi$ uniformly so 
  $\sup_\pi \E V_\pi \leq \E V^*$
  while trivially $\E V^* = \E V_{\pi^*} \leq \sup_\pi \E V_\pi$.
  For $Q_k^*$ the argument is similar.
\end{proof}

\begin{prop}
  $\sup_{a \in \Cal{A}} Q^*(s, a) = V^*(s)$
\end{prop}
\begin{proof}
  This is by definition after considering \cref{prop:Q*V*}.
\end{proof}

\begin{prop}
  $TQ^*_k = r + \gamma \E T V^*_k$ and if
  $\pi^* = (\pi^*_1, \dots, \pi^*_k, \dots)$
  is $k$-optimal then
  $Q^*_k = T_{\pi^*_1} \dots T_{\pi^*_k} r = T^k r$.
  \label{prop:Qk*}
\end{prop}
\begin{proof}
  \begin{align*}
    TQ^*_k(s,a) &= T(r + \gamma \E V^*_k)(s,a)
    \\ &= r(s,a) + \gamma
    \int \sup_{a' \in \Cal{A}} (r(s',a') + \gamma \E_{P(\cdot \mid s', a')} V^*_k)
    \difd P(s' \mid s,a)
    \\ &=r(s,a) + \gamma
    \int \sup_{a' \in \Cal{A}} \left(r(s',a') + \gamma
    \int V_k^*(s'') \difd P(s'' \mid s', a') \right)
    \difd P(s' \mid s,a)
    \\ &= r(s, a) + \gamma
    \int T V^*_k(s') \difd P(s' \mid s, a)
  \end{align*}
  To get $Q^*_k = T^k r$ use this inductively
  $Q^*_k = r + \gamma \E V^*_k = r+ \gamma TV^*_{k-1}
  = T Q^*_{k-1} = \dots$.
  For $Q^*_k = T_{\pi^*_1} \dots T_{\pi^*_k} r$ use
  $Q^*_k = r + \gamma \E V^*_k =
  r + \gamma T_{\pi^*_1} \dots T_{\pi^*_k} V_0$
  and first statement in \cref{prop:QpiVpi} inductively.
\end{proof}
The proof of \cref{prop:Qk*} also shows
\begin{prop}
  $TQ^* = r + \gamma \E T V^*$.
\end{prop}
implying
\begin{prop}
  $TQ^* = Q^*$.
  \label{prop:Q*FP}
\end{prop}

\begin{prop}
  For stationary $\pi \in S\Pi$ we have
  $T_\pi Q_\pi = Q_\pi$.
  \label{prop:QpiFP}
\end{prop}
\begin{proof}
  Using \cref{prop:QpiVpi} and \cref{prop:VpiFP}
  $T_\pi Q_\pi = T_\pi (r + \gamma \E \lim_{k\to\infty} T_\pi^k V_0)
  = \lim_{k\to\infty} T_\pi (r + \gamma \E T_\pi^k V_0)
  = \lim_{k\to\infty} (r + \gamma \E T_\pi^{k+1} V_0)
  = r + \gamma \E \lim_{k\to\infty} T^{k+1}_\pi V_0
  = r + \gamma \E V_\pi = Q_\pi$.
\end{proof}

\begin{prop}
  $Q^* = Q_{\pi^*}$ and
  $T_{\pi^*} Q^* = Q^*$.
  \label{prop:Q*=Qpi*}
\end{prop}
\begin{proof}
  $Q^* = r + \gamma \E V^* = r + \gamma \E V_{\pi^*} = Q_{\pi^*}$
  for the second statement use \cref{prop:QpiFP}.
\end{proof}

\begin{prop}
  $Q^* = \lim_{k\to\infty} Q_k^*$.
\end{prop}
\begin{proof}
  By monotone or dominated convergence and \cref{thm:BScor9.17.2}.
\end{proof}

\begin{prop}
  $T$ and $T_\pi$ is $\gamma$-contractive on
  $\Cal{L}_\infty(\Cal{S}\times\Cal{A})$.
  If furthermore (D) holds, then
  $\abs{Q_{k, \pi}}, \abs{Q_\pi}, \abs{Q^*_k}, \abs{Q^*} \leq V_{\max}$
  and $Q_\pi, Q^*$ are the unique fixed points of $T_\pi, T$ in 
  $\Cal{L}_\infty(\Cal{S}\times\Cal{A})$.
  \label{prop:Qcontracts}
\end{prop}

\begin{proof}
  The contrativeness of $T,T_\pi$ follows from the same argument as in
  \cref{prop:TpiContracts}. If (D) holds
  the boundedness of the $Q$ functions
  follow from an argument similar to the proof of \cref{prop:Vbounded}.
  Then \cref{prop:Q*FP}, \cref{prop:QpiFP} and Banach fixed point theorem
  implies uniqueness.
\end{proof}

\begin{prop}(D)

  For any $Q \in \Cal{L}_\infty(\Cal{S} \times \Cal{A})$
  $T^k Q$ converges to $Q^*$ with rate $\gamma^k$.

  That is
  $\norm{T^k Q - Q^*}_\infty \leq \gamma^k \norm{Q - Q^*}_\infty$.
  \label{prop:Qrate}
\end{prop}
\begin{proof} 
  By \cref{prop:Qcontracts} $T$ $\gamma$-contracts so
  \begin{align*}
    \norm{T^k Q - Q^*}_\infty &= \norm{T^k Q - T^k Q^*}
    \\ &\leq \gamma^k \norm{Q - Q^*}
  \end{align*}
\end{proof}

\begin{prop}(D)(N)

  $Q^*(s, \cdot)$ is upper semicontinuous.
\end{prop}
\begin{proof}
  Since $P$ is continuous and $V^*$ is upper semicontinuous by
  \cref{thm:BScor9.17.2} the proposition follow by
  \cref{prop:BS7_31}.
\end{proof}

\begin{defn}
  Let $\pi : \Cal{S} \to \Cal{A}$ be a stationary policy. If
  \[ \pi \left(\argmax_{a \in \Cal{A}} Q(s, a) \Mid s \right) = 1 \]
  then $\pi$ is said to be \defemph{greedy} with respect to $Q$ and is
  denoted $\pi_Q$.
\end{defn}

\begin{prop} (D)(N)

  Let $Q:\Cal{S} \times \Cal{A} \to \ol{\ul{\R}}$ be measurable
  and upper semicontinuous in the second entry.
  Then there exists a deterministic greedy policy for $Q$.
  \label{prop:greedyExistence}
\end{prop}
\begin{proof}
  Since $Q$ is upper semicontinuous in the second entry
  the set $A_s = \argmax_{a \in \Cal{A}} Q(s, a)$ is non-empty
  and measurable for all $s$.
  Pick (by axiom of choice) an $a_s \in A_s$ for every $s \in \Cal{S}$.
  Then $\pi(\cdot \mid s) = \delta_{a_s}$ is greedy with respect to $Q$.
  %todo is pi actually measurable?
\end{proof}

\begin{prop}
  For any $Q : \Cal{S} \times \Cal{A} \to \ol{\ul{\R}}$ if $\pi_Q$ is greedy
  with respect to $Q$ then $T_{\pi_Q} Q = TQ$.
  \label{prop:Tgreedy}
\end{prop}
\begin{proof}
  \begin{align*}
    T_{\pi_Q} Q &= r + \gamma \int Q(s, a) \difd \pi P(s, a \mid \cdot)
    \\ &= r + \gamma \int \int Q(s, a)
    \difd \pi_Q(a \mid s) \difd P(s \mid \cdot)
    \\ &= r + \gamma \int \max_{a \in \Cal{A}} Q(s, a)
    \difd P(s \mid \cdot)
    \\ &= T Q
  \end{align*}
\end{proof}

\begin{prop}
  Let $\pi_i$ be greedy with respect to $Q_{i-1}^*$ for $k \in \N$.
  Then $Q^*_k = T_{\pi_1} \dots T_{\pi_k} Q_0$ for all $k \in \N$.
  \label{prop:greedyOptFinite}
\end{prop}
%todo proof

\begin{prop}(D)(N)

  Any greedy policy with respect to $Q^*$ is
  optimal and can be chosen to be deterministic.
\end{prop}
\begin{proof}
  By \cref{prop:greedyExistence} we can pick a greedy policy $\pi$ for $Q^*$
  which can be chosen to be deterministic but let $\pi$ stay general.
  Then by \cref{prop:Tgreedy} $T_\pi Q^* = T Q^*$
  and by \cref{prop:Q*=Qpi*} $Q_\pi = Q^*$ implying that $\pi$ is optimal.
\end{proof}

\begin{algorithm}[H] %\label{algocf:fq} % this labels line, could not fix
\caption{Simple theoretical Q-iteration}
\KwIn{MDP $(\Cal{S}, \Cal{A}, P, R, \gamma)$, number of iterations $K$}
$\forall (s, a) \in \Cal{S} \times \Cal{A} :
r(s, a) \leftarrow \int x \difd R(x \mid s, a)$.

$\wt{Q}_0 \leftarrow r$

\For{$k = 0,1,2,\dots,K-1$}{
  $ \forall (s, a) \in \Cal{S} \times \Cal{A} :
  \wt{Q}_{k+1}(s, a) \leftarrow r(s, a)
  + \gamma \int \sup_{a' \in \Cal{A}} \wt{Q}_k(s', a') \difd P(s' \mid s, a)$
}
Define $\pi_K$ as the greedy policy w.r.t. $\wt{Q}_K$ \\
\KwOut{An estimator $\widetilde{Q}_K$ of $Q^*$ and policy $\pi_K$}
\label{alg:theoSimpleQ}
\end{algorithm}

\subsubsection{Finite Q-iteration}
Concluding on the results so far
we have showed how if one knows the process dynamics
of a stationary decision process satisfying rather broad criteria, 
such as continuity and compactness,
the optimal policy and state-value function can be found
simply by iteration over the $T$-operator and picking a greedy strategy
(see \cref{prop:Qrate}).
Of course this is practical computationally, only if
the resulting $Q$ functions can be represented and computed in finite
space and time.
This is trivially the case when
\begin{asm}
  $\Cal{S}\times\Cal{A}$ is finite.
  \label{asm:finite}
\end{asm}
Say $\abs{\Cal{S}} = k$ and $\abs{\Cal{A}} = \ell$.
In this case the transition operator $P$ can be represented as a
matrix of \emph{transition probabilities}
\[ P \defeq \begin{pmatrix}
    P(s_1 \mid s_1, a_1) & \dots & P(s_k \mid s_1, a_1)
    \\ \vdots & \vdots & \vdots
    \\ P(s_1 \mid s_k, a_\ell) & \dots & P(s_k \mid s_k, a_\ell)
\end{pmatrix} \]
then the algorithm becomes

\begin{algorithm}[H] %\label{algocf:fq} % this labels line, could not fix
\caption{Simple finite Q-iteration}
\KwIn{DP $(\Cal{S}, \Cal{A}, P, R, \gamma)$, number of iterations $K$}
Set $ r \leftarrow \left(\int r \difd R(\cdot \mid s_1, a_1),
\dots, \int r \difd R(\cdot \mid s_k, a_\ell) \right)^T $

and $ \wt{Q}_0 = r$.

\For{$k = 0,1,2,\dots,K-1$}{
  Set $m(\wt{Q}_k) \defeq (\max_{a \in \Cal{A}} Q(s_1, a), \dots,
  \max_{a \in \Cal{A}}Q(s_k, a))^T$

  Update action-value function:
  \[ \wt{Q}_{k+1} \leftarrow
    r + \gamma P m(\wt{Q}_k)
  \]
}
Define $\pi_K$ as the greedy policy w.r.t. $\wt{Q}_K$ \\
\KwOut{An estimator $\widetilde{Q}_K$ of $Q^*$ and policy $\pi_K$}
\label{alg:finiteSimpleQ}
\end{algorithm}

\begin{prop}
  The output $\wt{Q}_K$ from \cref{alg:finiteSimpleQ} is
  $K$-optimal and
  $\norm{\wt{Q}_K - Q^*}_\infty \leq \gamma^K \norm{Q^*}_\infty$.
\end{prop}
\begin{proof}
  See \cref{prop:Qrate} and \cref{prop:greedyOptFinite}.
\end{proof}


%todo check measurability (universal) issues in the above sections

\subsection{Approximation}

In this section we will look at what happens if we
instead use approximations the $Q$-functions and $T$ operator.
We first look at a naive approach using $Q$-functions.

Let $\wt{Q}_0$ be any bounded Q-function.
Suppose we approximate $T\wt{Q}_0$ by a Q-function $\wt{Q}_1$
to $\ve_1 > 0$ precision and then approximate $T\wt{Q}_1$ and so on
getting a sequence of Q-functions satisfying
\[ \abs{T\wt{Q}_{k-1} - \wt{Q}_k} \leq \ve_k, \forall k \in \N \]

First observe that
\begin{align*}
  \abs{T^k \wt{Q}_0 - \wt{Q}_k}
  &\leq \abs{T^k \wt{Q}_0 - T \wt{Q}_{k-1}} + \abs{T\wt{Q}_{k-1} - \wt{Q}_k}
  \\ &\leq \gamma \abs{T^{k-1} \wt{Q}_0 - \wt{Q}_{k-1}}
  + \abs{T\wt{Q}_{k-1} - \wt{Q}_k}
\end{align*}

Using this iteratively we get
\[ \abs{T^k \wt{Q}_0 - \wt{Q}_k} \leq \sum_{i=1}^k \gamma^{k-i} \ve_i
\defeq \ve_a(k) \]

Then we can bound
\begin{align*}
  \abs{Q^* - \wt{Q}_k}
  &\leq \abs{Q^* - T^k \wt{Q}_0} + \abs{T^k \wt{Q}_0 - \wt{Q}_k}
  \\ &\leq \gamma^k \abs{Q^* - \wt{Q}_0}
  + \ve_a(k)
\end{align*}

The first term converges quickly while the other depends on our
step-wise approximations. For example
$\ve_i(k) = \ve$ we easily get the bound
$ \ve_a(k) = \ve \frac{1-\gamma^k}{1-\gamma} \leq \frac{\ve}{1-\gamma} $
Or if $\ve_i \leq c\gamma^i$ we get $\ve_a(k) \leq ck \gamma^k \to 0$ as
$k \to \infty$.
Generally if one can show that $\ve_i \to 0$ we have
\begin{prop} $ \sum_{i-1}^k \gamma^{k-i} \ve_i \to 0 $
  whenever $\ve_k \to 0$ as $k \to \infty$.
\end{prop}
\begin{proof}
  Let $\ve > 0$. Find $N$ such that $\ve_n \leq \ve (1-\gamma)/2$ 
  for all $n>N$ and find $M>N$ such that
  $\gamma^M \leq
  \ve \gamma^N \left( \sum_{i=1}^N \gamma^{N-i} \ve_i \right)^{-1}$.
  Then for all $m>M$
  \begin{align*}
    \sum_{i=1}^m \gamma^{m-i} \ve_i
    &\leq \gamma^{m-N} \sum_{i=1}^N \gamma^{N-i} \ve_i
    + \sum_{i=N+1}^m \gamma^{m-i} \ve (1-\gamma)/2
    \leq \ve/2 + \ve/2 \leq \ve
  \end{align*}
\end{proof}


