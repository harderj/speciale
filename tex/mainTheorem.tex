\begin{thm}[Yang, Xie, Wang] \label{thm:main}
	For any $K \in \N$ let $Q^{\pi_K}$ be the action-value function
	corresponding to policy $\pi_K$ which is returned by Algorithm 1,
	when run with a sparse ReLU network on the form
	\[ \Cal{F}_0 = \{f(\cdot, a) \in \Cal{F}(L^*, \{d_j^*\}_{j=0}^{L^*+1},s^*)
		\mid a \in \Cal{A} \} \]
	where
	\[ L^* \lesssim (\log n)^{\xi'}, d_0 = r, d_j^*, d_{L+1}=1, \lesssim n^{\xi'},
		s^* \asymp n^{\alpha^*} \cdot (\log n)^{\xi'} \]
	Let $\mu$ be any distribution over $\Cal{S} \times \Cal{A}$.
	Under \cref{asm:A1} and \cref{asm:A2} 
	\[ \norm{Q^* - Q^{\pi_K}}_{1,\mu} \leq C \cdot \frac{\phi_{\mu,\nu}
			\cdot \gamma}{(1-\gamma)^2}
			\cdot |\Cal{A}| \cdot (\log n)^{\xi^*} \cdot n^{(\alpha^* - 1)/2} 
			+ \frac{4 \gamma^{K+1}}{(1-\gamma)^2} \cdot R_{\max} \]
	Here $C, \xi', \xi^*, \phi_{\mu,\nu} \in \R_{+}$ and $\alpha^* \in (0,1)$
	are constants depending on the assumptions
	and $R_{\max}$ the maximum possible reward.
\end{thm}


