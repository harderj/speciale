\subsection{Reinforcement Learning}
In Reinforcement Learning (RL) we are concerned with finding an optimal policy
for an agent in some environment.
Typically (also in the case of Q-learning) this environment is a
Markov decision process

\begin{defn}
	A Markov decision process (MDP) $(\Cal{S}, \Cal{A}, P, R, \gamma)$
	consists of
\begin{itemize}
\item $\Cal{S}$ a set of states
\item $\Cal{A}$ a set of actions
\item $P : \Cal{S} \times \Cal{A} \to \Cal{P}(\Cal{S})$ its Markov transition kernel
\item $R : \Cal{S} \times \Cal{A} \to \Cal{P}(\R)$ its immediate reward distribution
\item $\gamma \in (0,1)$ the discount factor
\end{itemize}
\end{defn}

A policy (for an MDP) is a function
\[\pi : \Cal{S} \to \Cal{P}(\Cal{A})\]
With this we can define the state-value function $V^\pi : \Cal{S} \to \R$
\[ V^\pi(s) = \E \left( \sum_{t \geq 0} \gamma^t R_t \mid
R_t \sim R(S_t, A_t), S_t \sim P(S_{t-1}, A_{t-1}), A_t \sim \pi(S_t), S_0 = s
\right) \]
And the state-action-value (Q-) function $Q^\pi : \Cal{S} \times \Cal{A} \to \R$
\[ Q^\pi(s, a) = \E (R(s, a) + \gamma V^\pi(S_0) \mid S_0 \sim P(s, a)) \]
The optimal Q-function is defined as
\[ Q^*(s, a) = \sup_\pi Q^\pi(s, a) \]
One can show that there is a policy $\pi^*$ such that $Q^* = Q^{\pi^*}$.
This is the optimal policy - the goal of RL. 

Note that $V^\pi$, $Q^\pi$ and $Q^*$ are usually infeasible to calculate to
machine precision, unless $\Cal{S} \times \Cal{A}$ is finite and not very big.

\subsection{Q-Learning}

Let $\pi: \Cal{S} \to \Cal{P}(\Cal{A})$ be a policy. We define the operator
\[ (P^\pi Q)(s, a) = \E (Q(S', A') \mid S' \sim P(s, a), A' \sim \pi(S')) \]
Intuitively this operator yields the expected state-action-value function
when looking \emph{one step ahead} following the policy $\pi$ and taking
expectation of $Q$.

We define the operator $T^\pi$ called the Bellman operator by
\[ (T^\pi Q)(s, a) = \E R(s, a) + \gamma (P^\pi Q)(s, a) \]
This operator adjust the $Q$ function to look more like $Q^\pi$ making one
"iteration" of "propagation of rewards" discounting with $\gamma$.
Indeed it is easily seen that $Q^\pi$ is a fixed point for $T^\pi$.

A \emph{greedy} policy $\pi$ with respect to a state-action value function $Q$
is a policy which deterministically chooses an action with maximal value of $Q$
for each state.
That is $\pi(s) = \delta_a$ for some $a \in \argmax_a Q(s,a)$.
We then write $\pi = \pi_Q$. With this we can define the operator $T$:
\[ T Q = T^{\pi_Q} Q \]
called the Bellman \emph{optimality} operator.

The Bellman optimality \emph{equation} can then be written $Q^* = TQ^*$.

\input{resultsInQLearning}

\subsection{Artificial Neural Networks}

\begin{defn}\label{def_ANN}
	An \textbf{ANN} (Artificial Neural Network) with structure
	$\{ d_i \}_{i=0}^{L+1} \subseteq \N$,
	activation functions $\sigma_i = (\sigma_{ij} : \R \to \R)_{j=1}^{d_i}$
	and weights $\{ W_i \in M^{d_i \times d_{i-1}}, v_i \in \R^{d_i} \}_{i=1}^{L+1}$
	is the function $F:\R^{d_0} \to \R^{d_{L+1}}$ 
	\[ F = w_{L+1} \circ \sigma_L \circ w_L \circ \sigma_{L-1} \circ \dots \circ w_1 \]
	where $w_i$ is the affine function $x \mapsto W_i x + v_i$ for all $i$.

	Here $\sigma_i(x_1, \dots, x_{d_i})
	= (\sigma_{i1}(x_1), \dots, \sigma_{id_{i}}(x_{d_{i}}))$.

	$L \in \N_0$ is called the number of hidden layers.

	$d_i$ is the number of neurons or nodes in layer $i$.
\end{defn}

An ANN is called \emph{deep} if there are two or more hidden layers.

\subsection{Fitted Q-Iteration}

We here present the algorithm which everything in this paper revolves around:

\begin{algorithm}[H] %\label{algocf:fq} % this labels line, could not fix
	\caption{Fitted Q-Iteration Algorithm}
	\KwIn{MDP $(\Cal{S}, \Cal{A}, P, R, \gamma)$, function class $\Cal{F}$,
		sampling distribution $\nu$, number of iterations $K$,
		number of samples $n$, initial estimator $\widetilde{Q}_0$}
	\For{$k = 0,1,2,\dots,K-1$}{
		Sample i.i.d. observations $\{(S_i, A_i), i \in [n]\}$ from $\nu$
		obtain $R_i \sim R(S_i, A_i)$ and $S'_i \sim P(S_i, A_i)$ \\
		Let $Y_i = R_i + \gamma \cdot \max_{a \in \Cal{A}} \widetilde{Q}_k(S'_i, a)$ \\
		Update action-value function:
		\[ \widetilde{Q}_{k+1} \leftarrow
			\argmin_{f \in \Cal{F}} \frac{1}{n}
			\sum_{i=1}^n (Y_i - f(S_i, A_i))^2 \]
	}
	Define $\pi_K$ as the greedy policy w.r.t. $\widetilde{Q}_K$ \\
	\KwOut{An estimator $\widetilde{Q}_K$ of $Q^*$ and policy $\pi_K$}
\end{algorithm}



