
Throughout we are working with a background probability space
denoted $(\Omega, \Cal{H}, \Prob)$.

\subsection{Reinforcement Learning}

Through the  probability space, denote this

In Reinforcement Learning (RL) we are concerned with finding an optimal policy
for an agent in some environment.
Typically (also in the case of Q-learning) this environment is a
Markov decision process

\begin{defn}
	A Markov decision process (MDP) $(\Cal{S}, \Cal{A}, P, R, \gamma)$
	consists of
\begin{itemize}
  \item $(\Cal{S}, \Sigma_{\Cal{S}})$ a measurable space of \defemph{states}.
  \item $(\Cal{A}, \Sigma_{\Cal{A}})$ a measurable space of \defemph{actions}.
  \item $P(\cdot \mid \cdot) : \Sigma_{\Cal{S}} \times (\Cal{S} \times \Cal{A})
    \to [0,1]$ a probability kernel (of \defemph{transition} probabilities).
  \item $R(\cdot \mid \cdot) : \bb{B} \times (\Cal{S} \times \Cal{A})
    \to [0,1]$ a probability kernel (of \defemph{reward} probabilities).
  \item $\gamma \in (0,1)$ a \defemph{discount} factor.
\end{itemize}
\end{defn}

In order for this to make sense we here include
\begin{defn}[Probability kernel]
  Let $(X, \Sigma_X), (Y, \Sigma_Y)$ be measurable spaces.
  A function $\kappa(\cdot \mid \cdot) : \Sigma_Y \times X \to [0,1]$
  is a \defemph{probability kernel} provided
  \begin{itemize}
    \item $B \mapsto \kappa(B \mid x) \in \Cal{P}(\Sigma_Y)$
      that is $\kappa(\cdot \mid x)$ is a probability measure
      for any $x \in X$.
    \item $x \mapsto \kappa(B \mid x) \in \Cal{M}(\Sigma_X, \Sigma_Y)$
      that is $\kappa(B \mid \cdot)$ is ($\Sigma_X/\Sigma_Y$-) measurable
      for any $B \in \Sigma_Y$.
  \end{itemize}
\end{defn}

Note that both $P$ and $R$ to be stochastic
and that $R$ can depend on the action as well as the state.
This is perhaps the most general way to define an MDP,
generalizing some definitions. Common variations include that
$R$ depends on $\Cal{S}$ only,
$R$ is deterministic, or
$P$ is deterministic. %todo backup this claim by a reference or three
\begin{defn}[Policy]
A (\defemph{randomized}, \defemph{stationary}) \defemph{policy}
$\pi$ is probability kernel
\[\pi(\cdot \mid \cdot) : \Sigma_{\Cal{A}} \times \Cal{S} \to [0,1] \]
\end{defn}

An MDP together with a policy and an initial distribution
$\mu \in \Cal{P}(\Cal{S})$
give rise to a countable stochastic process,
$ (X_i)_{i\in \N} = (S_i, A_i, R_i)_{i\in \N}$ 
that is a probability measure $P^\pi_\mu$. on
$(\Cal{S} \times \Cal{A} \times \R)^\N $.
See [ref. to Feinberg On Meas. and Repr. of Str. Meas. p. 31-32,
and pos. Ionescu Tulcea] %todo ref
for full details of how this is constructed.
Intuitively $S_1$ is drawn from $\mu$,
then for all $i \in \N$
$A_i$ is drawn from $\pi(\cdot \mid S_i)$,
a reward is then drawn from $R(\cdot \mid S_i, A_i)$,
then $S_{i+1}$ is drawn from $P(\cdot \mid S_i, A_i)$ and so on.
%todo: explain more about measures
We let $E^\pi_\mu$ denote the expectation taken w.r.t $P^\pi_\mu$.
When $\mu$ is a Dirac measure $\delta_x$, i.e. $\mu(\{x\}) = 1$ for some $x$,
we shall generally write $x$ instead of $\mu$,
E.g. $\E^\pi_s$ the expectation taken with respect to $P^\pi_{\delta_s}$

\subsection{Q-Learning}

Fix an MDP $(\Cal{S}, \Cal{A}, P, R, \gamma)$ and a policy $\pi$.
We assume from now on that any $R \sim R(\cdot \mid s, a)$ is bounded
with $\abs{R} \leq R_{\max}$ for any $(s,a) \in \Cal{S} \times \Cal{A}$.

Generally in value iteration and Q-learning,
any function $\Cal{S} \to \R$ is called a (\defemph{state})
\defemph{value} function. Similarly any function
$\Cal{S} \times \Cal{A} \to \R$ is called a (\defemph{state})
\defemph{action value} or \defemph{Q}- function.
The idea behind such functions are (usually) to estimate the
cumulative rewards associated with a state or state-action pair
and the trajectory of states it can lead to.
One has the reduced the problem of finding a good strategy
to choosing the action that leads to the
highest value. Of course this value of a state will depend
strongly on the policy being followed. 

The \defemph{ideal} value function
w.r.t. $\pi$,
$V^\pi : \Cal{S} \to \R$
\[ V^\pi(s) \defeq \E^\pi_s \sum_{t = 1}^\infty \gamma^t R_t \]
where $R_t$ are the projections of the random process generated from the
MDP with starting distribution $\delta_s$ onto the rewards of each step.
This is well-define because each $R_t$ is bounded by $R_{\max}$ so
$V^{\pi}(s) \leq \sum_{t=0}^\infty \gamma^t R_{\max}
= \frac{1}{1-\gamma} R_{\max} := V_{\max} < \infty $.
The value function gives the expected 
accumulated reward when starting in state $s$ and following policy $\pi$.

The \defemph{ideal} Q-function 
w.r.t. $\pi$,
$Q^\pi : \Cal{S} \times \Cal{A} \to \R$
\[ Q^\pi(s, a) \defeq r(s, a) +
\gamma \E (V^\pi(S) \mid S \sim P(\cdot \mid s, a)) \]
where $r(s,a) \defeq \E(R \mid R \sim R(\cdot \mid s, a))$.

One could think that it is a bit superfluous to define both a value and an
action value function. 
%todo: prove something about the equivalence of value-iteration and Q-iteration
According to [todo: ref] the main reason to work with Q-functions
is that it is more difficult to work with value function for several reasons.
Firstly to know what is the best action $a^*$ given a state $s$
and a value function $V$, one has to calculate for each action $a$
the distribution of the next state $s'$ and take expectation over
$V(s')$. This either requires full knowledge of the transition kernel
(this falls outside the so called model-free approaches)
or some way of estimating it, and in both cases at computational cost.
Whereas Q-functions simply requires finding
$\argmax_{a \in \Cal{A}} Q(s,a)$.
%todo: find source comparing convergence of value-iteration and Q-iteration

We define the operator $P^\pi$
\[ (P^\pi Q)(s, a) \defeq \E (Q(S', A') \mid S' \sim P(\cdot \mid s, a),
A' \sim \pi(\cdot \mid S')) \]
Intuitively this operator yields the expected action value function
when looking \emph{one step ahead} following the policy $\pi$ and taking
expectation of $Q$.
Note that $\norm{P^\pi Q}_\infty \leq \norm{Q}_\infty$.
$\norm{\cdot}_\infty$.

We define the operator $T^\pi$ called the Bellman operator by
\[ (T^\pi Q)(s, a) \defeq r(s, a) + \gamma (P^\pi Q)(s, a) \]

The Bellman operator adjusts an action value function $Q$
to look more like $Q^\pi$. This is intuitively by making one
iteration of reward-propagation discounting with $\gamma$.
And indeed 
%todo: prove this
\begin{prop}
  $Q^\pi$ is the unique fixed point of $T^\pi$. 
  \label{prop:introQfixT}
\end{prop}
\begin{proof}
  $Q^\pi$ is a fixed point because for any $s \in \Cal{S}, a \in \Cal{A}$
  \begin{align*}
    T^\pi Q^\pi (s, a)
    = & \; r(s, a) + \gamma (P^\pi Q^\pi)(s, a)
    \\ = & \; r(s, a) + \gamma
    \E(Q^\pi(S',A') \mid S' \sim P(\cdot \mid s, a), A' \sim \pi(\cdot, S'))
    \\ = & \; r(s, a) + \gamma
    \E( r(S', A') + \gamma \E(V^\pi(S'') \mid S'' \sim (\cdot \mid S', A'))
    \mid S' \sim P(\cdot \mid s, a), A' \sim \pi(\cdot, S'))
    \\ = & \; r(s,a) + \gamma \E(V^\pi(S') \mid S' \sim P(\cdot \mid s, a))
    \\ = & \; Q^\pi(s, a)
  \end{align*}
  Now since
  \begin{align*}
    Q^\pi - T^\pi Q = T^\pi Q^\pi - T^\pi Q = \gamma P^\pi (Q^\pi - Q)
  \end{align*}
  by induction
  \begin{align*}
    Q^\pi - (T^\pi)^n Q = (\gamma P^\pi)^n (Q^\pi - Q)
  \end{align*}
  And since $\gamma P^\pi$ contracts to 0,
  in fact every bounded Q-function converges to $Q^\pi$ when iteratively applying
  $T^\pi$. In particular $Q^\pi$ is the only fixed point of $T^\pi$.
\end{proof}

If an action value function $Q$ satisfies that
$Q(s, \Cal{A})$ has a greatest value for every $s \in \Cal{S}$ then we can define
\defemph{greedy} policy $\pi$ with respect to $Q$
to be a policy choosing an action with maximal value of $Q$ for each state.
That is $\pi(s) = \delta_a$ for some $a \in \argmax_a Q(s,a)$.
We then write $\pi = \pi_Q$.

Let $\pi_0$ be a policy and $Q_0 = Q^{\pi_0}$ be its ideal Q-function.
One can now consider the greedy policy $\pi_1 = \pi_{Q_0}$.
Note that
\begin{align}
  T^{\pi_1} Q^{\pi_0} = & \; r(s, a) + \gamma \E \left(
    \sup_{a' \in \Cal{A}} Q^{\pi_0}(S', a') \mid S' \sim P(\cdot \mid s, a) 
  \right)
  \\ \geq & \; r(s, a) + \gamma \E \left(
    Q^{\pi_0}(S', A') \mid S' \sim P(\cdot \mid s, a), A' \sim \pi_0(\cdot \mid S')
  \right)
  \\ = & \; Q^{\pi_0} \label{eq:introTlem1}
\end{align}
so applying $T^{\pi_1}$ iteratively on $Q_0$ creates a 
monotonically increasing sequence of Q-functions
which by \cref{prop:introQfixT} converge to $Q^{\pi_1}$,
proving that $Q^{\pi_1} \geq Q^{\pi_0}$.
One can then repeat the process with $\pi_1$ and
obtain a increasing sequences of (ideal) Q-functions with associated policies
$(Q_0,Q_1, \dots)$, $(\pi_0, \pi_1, \dots)$.
Variations of this idea is called \defemph{policy iteration}
and has been studied a lot.
%todo results about policy iteration.
Variations include stopping the ``value iteration''
(applying the $T^{\pi_i}$ operator) at various stages before again updating
the policy to be greedy w.r.t. to the next Q-function.
An important special case is where we simply alternate between updating
the policy and the Q-function in every step.
We can capture this in a single operator called the
Bellman \emph{optimality} operator $T$, defined as
\[ T Q \defeq T^{\pi_Q} Q \]

The optimal Q-function is defined as
\[ Q^*(s, a) \defeq \sup_\pi Q^\pi(s, a) \]
where the supremum is taken over all policies.

%todo: discuss existence of optimal policy
%\begin{prop}
%  There exists a policy $\pi^*$ such that $Q^* = Q^{\pi^*}$
%  called the optimal policy.
%\end{prop}
%\begin{proof}
%  Let $\pi^* = \pi_{Q^*}$. By \cref{eq:introTlem1}
%\end{proof}
%One can show that there is a policy $\pi^*$ such that $Q^* = Q^{\pi^*}$.
%This is the optimal policy - the goal of RL.

%The Bellman optimality \emph{equation} can then be written $Q^* = TQ^*$.

Note that $V^\pi$, $Q^\pi$ and $Q^*$ are usually infeasible to calculate to
machine precision, unless $\Cal{S} \times \Cal{A}$ is finite and not very big.

\input{resultsInQLearning}

\subsection{Artificial Neural Networks}

\begin{defn}\label{def_ANN}
	An \textbf{ANN} (Artificial Neural Network) with structure
	$\{ d_i \}_{i=0}^{L+1} \subseteq \N$,
	activation functions $\sigma_i = (\sigma_{ij} : \R \to \R)_{j=1}^{d_i}$
	and weights $\{ W_i \in M^{d_i \times d_{i-1}}, v_i \in \R^{d_i} \}_{i=1}^{L+1}$
	is the function $F:\R^{d_0} \to \R^{d_{L+1}}$ 
	\[ F = w_{L+1} \circ \sigma_L \circ w_L \circ \sigma_{L-1} \circ \dots \circ w_1 \]
	where $w_i$ is the affine function $x \mapsto W_i x + v_i$ for all $i$.

	Here $\sigma_i(x_1, \dots, x_{d_i})
	= (\sigma_{i1}(x_1), \dots, \sigma_{id_{i}}(x_{d_{i}}))$.

	$L \in \N_0$ is called the number of hidden layers.

	$d_i$ is the number of neurons or nodes in layer $i$.
\end{defn}

An ANN is called \emph{deep} if there are two or more hidden layers.

\begin{defn}[Sparse ReLU Networks]
  For $s,V \in \R$ a (s,V)-\defemph{Sparse ReLU Network} is an ANN $f$
  with any structure $\{d_i\}_{i\in [L+1]}$,
  all activation functions being \emph{ReLU} i.e. $\sigma_{ij} = \max(\cdot, 0)$
  and any weights $(W_\ell, v_\ell)$
  satisfying
  \begin{multicols}{3}
    \begin{itemize}
      \item $\max_{\ell \in [L+1]} \norm{\widetilde{W}_\ell}_\infty \leq 1$
      \item $\sum_{\ell = 1}^{L+1} \norm{\widetilde{W}_\ell}_0 \leq s$
      \item $\max_{j \in [d_{L+1}]} \norm{f_j}_\infty \leq V$
    \end{itemize}
  \end{multicols}
  Here $\widetilde{W}_\ell = (W_\ell, v_\ell)$.
  The set of them we denote $\Cal{F}\left(L, \{d_i\}_{i=0}^{L+1},s,V \right)$.
  \label{def:sparseReLU}
\end{defn}

\subsection{Fitted Q-Iteration}

We here present the algorithm which everything in this paper revolves around:

\begin{algorithm}[H] %\label{algocf:fq} % this labels line, could not fix
  \caption{Fitted Q-Iteration Algorithm}
  \KwIn{MDP $(\Cal{S}, \Cal{A}, P, R, \gamma)$, function class $\Cal{F}$,
    sampling distribution $\nu$, number of iterations $K$,
  number of samples $n$, initial estimator $\widetilde{Q}_0$}
  \For{$k = 0,1,2,\dots,K-1$}{
    Sample i.i.d. observations $\{(S_i, A_i), i \in [n]\}$ from $\nu$
    obtain $R_i \sim R(S_i, A_i)$ and $S'_i \sim P(S_i, A_i)$ \\
    Let $Y_i = R_i + \gamma \cdot \max_{a \in \Cal{A}} \widetilde{Q}_k(S'_i, a)$ \\
    Update action-value function:
    \[ \widetilde{Q}_{k+1} \leftarrow
      \argmin_{f \in \Cal{F}} \frac{1}{n}
    \sum_{i=1}^n (Y_i - f(S_i, A_i))^2 \]
  }
  Define $\pi_K$ as the greedy policy w.r.t. $\widetilde{Q}_K$ \\
  \KwOut{An estimator $\widetilde{Q}_K$ of $Q^*$ and policy $\pi_K$}
  \label{alg:fqi}
\end{algorithm}



