
\subsubsection{Reinforcement learning}

In Reinforcement Learning (RL) we are concerned with finding an optimal policy
for an agent in some environment.
This environment is described by a so-called decision process
consisting of a sequence of state and action spaces
$\Cal{S}_1, \Cal{A}_1, \Cal{S}_2, \dots$
and rules $P_1, R_1, P_2, \dots$ specifying which states and rewards
and likely to follow after some action is chosen.
In the general case $\Cal{S}_i, \Cal{A}_i$ are measurable spaces
and $P_i, R_i$ are probability kernels on $S_{i+1}$ and $\Rext$, respectively.
One then attempts to find a \emph{policy} (behavior or strategy) that
maximizes the rewards returned from the environment.
A policy and an distribution over starting states
$\mu \in \Cal{P}(\Cal{S}_1)$
give rise to a countable stochastic process,
$ (X_i)_{i\in \N} = (S_i, A_i, R_i)_{i\in \N}$ 
that is a probability measure $P^\pi_\mu$. on
$\Cal{S}_1 \times \Cal{A}_1 \times \dots$.
Intuitively $S_1$ is drawn from $\mu$,
then for all $i \in \N$
$A_i$ is drawn from $\pi(\cdot \mid S_i)$,
a reward is then drawn from $R(\cdot \mid S_i, A_i)$,
then $S_{i+1}$ is drawn from $P(\cdot \mid S_i, A_i)$ and so on.


In RL there is a categorization of algorithms into
\emph{off-policy} and \emph{on-policy} classes.
This is simply whether the algorithm learns from data
(states, actions and rewards) arising from 
following its own policy (on-policy) or it can learn from more
arbitrary data (off-policy).
This \emph{more arbitrary data} could for example be
the trajectory of another algorithm when interacting with a decision process,
or simply state-action-reward pairs drawn from some distribution.
In this paper we exclusively consider off-policy algorithms.
