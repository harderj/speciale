
In RL there is a categorization of algorithms into
\emph{off-policy} and \emph{on-policy} classes.
This is simply whether the algorithm learns from data
(states, actions and rewards) arising from 
following its own policy (on-policy) or it can learn from more
arbitrary data (off-policy).
This \emph{more arbitrary data} could for example be
the trajectory of another algorithm when interacting with a decision process,
or simply state-action-reward pairs drawn from some distribution.
In this paper we exclusively consider off-policy algorithms.
