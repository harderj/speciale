
\documentclass{article}

\input{../preamble}

\begin{document}


\subsection{Measure Theory}

We work with a background probability space $(\Omega, \Sigma_\Omega, \Prob)$.
For a measurable space $(\Cal{X}, \Sigma_{\Cal{X}})$ we denote
the set of probability measures on this space $\Cal{P}(\Sigma_\Cal{X})$ or
simply $\Cal{P}(\Cal{X})$ when the $\sigma$-algebra is unambiguous.
When taking cartesian products $\Cal{X} \times \Cal{Y}$ of measurable spaces
$(\Cal{X}, \Sigma_\Cal{X}), (\Cal{Y}, \Sigma_\Cal{Y})$ we always endow such
with the product $\sigma$-algebra $\Sigma_\Cal{X} \otimes \Sigma_\Cal{Y}$,
unless otherwise specified.
A map $f: \Cal{X} \to \Cal{Y}$ is called $\Sigma_{\Cal{X}}$-$\Sigma_{\Cal{Y}}$
measurable provided $f^{-1}(\Sigma_{\Cal{Y}}) \subseteq \Sigma_{\Cal{X}}$
and we denote the set of such functions $\Cal{M}(\Sigma_{\Cal{X}},
\Sigma_{\Cal{Y}})$.
By a random variable $X$ on $(\Cal{X}, \Sigma_{\Cal{X}})$ mean a
$\Sigma_\Omega$-$\Sigma_{\Cal{X}}$ measurable map.

\subsubsection{Kernels}

\begin{defn}[Probability kernel]
  Let $(\Cal{X}, \Sigma_\Cal{X}), (Y, \Sigma_\Cal{Y})$ be measurable spaces.
  A function
  \[ \kappa(\cdot \mid \cdot) : \Sigma_\Cal{Y} \times \Cal{X} \to [0,1] \]
  is a $(\Cal{X}, \Sigma_\Cal{X})$-\defemph{probability kernel}
  on $(\Cal{Y}, \Sigma_\Cal{Y})$ provided
  \begin{enumerate}
    \item $B \mapsto \kappa(B \mid x) \in \Cal{P}(\Sigma_\Cal{Y})$
      that is $\kappa(\cdot \mid x)$ is a probability measure
      for any $x \in \Cal{X}$.
    \item
      $x \mapsto \kappa(B \mid x) \in \Cal{M}(\Sigma_\Cal{X}, \Sigma_\Cal{Y})$
      that is $\kappa(B \mid \cdot)$ is ($\Sigma_\Cal{X}$-$\Sigma_\Cal{Y}$)
      measurable for any $B \in \Sigma_\Cal{Y}$.
  \end{enumerate}
  When the $\sigma$-algebras are unambiguous we shall simply say an
  $\Cal{X} \leadsto \Cal{Y}$ kernel.
  For any $x \in \Cal{X}$ and $f \in \Cal{L}_1(\kappa(\cdot \mid x))$
  we write the integral of $f$ over $\kappa(\cdot \mid x)$ as
  $\int f(y) \difd \kappa(y \mid x)$.
  \label{defn:probKer}
\end{defn}

We now state some fundamental results on probability kernels
\begin{thm}[Integration of a kernel]
  Let $\mu \in \Cal{P}(\Cal{X})$ and $\kappa : \Cal{X} \leadsto \Cal{Y}$.
  Then there exists a uniquely determined probability measure
  $\lambda \in \Cal{P}(\Sigma_\Cal{X} \otimes \Sigma_\Cal{Y})$
  such that
  \[ \lambda(A \times B) = \int_A \kappa(B, x) \difd \mu(x) \]
  \label{thm:intKer}
  We denote this measure $\lambda = \kappa \mu$.
\end{thm}
\begin{proof}
  We refer to [ref to EH markov, thm. 1.2.1].
\end{proof}

Notice that by \cref{thm:intKer}
besides getting a probability measure on $\Cal{X} \times \Cal{Y}$
we get an induced probability measure
on $\Cal{Y}$ defined by $B \mapsto (\kappa \mu)(\Cal{X} \times B)$.
We will denote this measure by $\kappa \circ \mu$.
This way $\kappa$ can also be seen as a mapping from
$\Cal{P}(\Cal{X}) \to \Cal{P}(\Cal{Y})$.
Also note that $\kappa \circ \delta_x = \kappa(\cdot \mid x)$.

For an idea how to actually compute integrals over kernel derived measures
we here include
\begin{thm}[Extended Tonelli and Fubini]
  Let $\mu \in \Cal{P}(\Cal{X})$,
  $f \in \Cal{M}(\Sigma_\Cal{X} \otimes \Sigma_\Cal{Y}, \bb{B})$
  be a measurable function and
  $\kappa : \Cal{X} \leadsto \Cal{Y}$ be a probability kernel.
  Then
  \[ \int \abs{f} \difd \kappa \circ \mu
  = \int \int \abs{f} \difd \kappa(\cdot \mid x) \difd \mu(x) \]
  Furthermore if this is finite, i.e. $f \in \Cal{L}_1(\kappa(\cdot, \mu))$
  then $A_0 \defeq \left\{ x \in \Cal{X} \Mid
    \int f \difd \kappa(\cdot \mid x) < \infty \right\}
  \in \Sigma_\Cal{X}$
  with $\mu(A_0) = 1$, 
  \[ x \mapsto \begin{cases}
      \int f \difd \kappa(\cdot \mid x) & x \in A_0
      \\ 0 & x \not\in A_0
  \end{cases} \]
  is $\Sigma_\Cal{X}$-$\bb{B}$ measurable and
  \[ \int f \difd \kappa(\cdot \mid \mu)
  = \int_{A_0} \int f \difd \kappa(\cdot \mid x) \difd \mu(x) \]  
  \label{thm:extTonFub}
\end{thm}
\begin{proof}
  We refer to [ref to EH markov, thm. 1.3.2 + 1.3.3]
\end{proof}

\begin{prop}[Composition of kernels]
  Let $\kappa : \Cal{X} \leadsto \Cal{Y}, \psi : \Cal{Y} \leadsto \Cal{Z}$
  be probability kernels. Then
  \[ (\psi \circ \kappa)(A \mid x) \defeq
    \int \psi(A \mid y) \difd \kappa(y \mid x)
  ,\qquad \forall A \in \Sigma_{\Cal{Z}}, x \in \Cal{X} \]
  is a $\Cal{X} \leadsto \Cal{Z}$ probability kernel called the
  composition of $\kappa$ and $\psi$. The composition operator
  $\circ$ is associative, i.e. if $\phi : \Cal{Z} \leadsto \Cal{W}$ is
  a third probability kernel then $(\phi \circ \psi) \circ \kappa = 
  \phi \circ (\psi \circ \kappa)$.
  The associativity also extends to measures, i.e.
  $\forall \mu \in \Cal{X}
  : (\psi \circ \kappa) \circ \mu = \psi \circ (\kappa \circ \mu) $
  and this is uniquely determined by $\psi, \kappa$ and $\mu$.
  \label{prop:compKer}
\end{prop}
\begin{proof}
  The first assertion is a trivial verification of the two conditions
  in \cref{defn:probKer} and left as an exercise.
  For the associativity we refer to [todo ref to EH markov, lem. 4.5.4].
\end{proof}

\Cref{prop:compKer} actually makes the class of measurable spaces
into a category [todo ref: see Lawvere, The Category of Probabilistic
Mappings], with identity $\id_{\Cal{X}}(\cdot \mid x) = \delta_x$.
Notice that the mapping $(A, x) \mapsto \delta_x(A) \kappa(A \mid x)$
defines a probability kernel $\Cal{X} \leadsto \Cal{X} \times \Cal{Y}$
which we could denote $\id_{\Cal{X}} \times \kappa$.
Now if $\psi : \Cal{X} \times \Cal{Y} \leadsto \Cal{Z}$ is a kernel
then by \cref{prop:compKer} the composition
$(\id_{\Cal{X} \times \Cal{Y}} \times \psi)
\circ (\id_{\Cal{X}} \times \kappa)$
is a kernel $\Cal{X} \to \Cal{X} \times \Cal{Y} \times \Cal{Z}$
which we will denote $\psi \kappa$.
It inherits associativity from $\circ$ and again this associativity
extends to application on measures: if $\mu$ is a measure on $\Cal{X}$
then $\psi (\kappa \mu) = (\psi \kappa) \mu$.

\begin{prop}
  Let $\kappa : \Cal{X} \to \Cal{Y}$ be a probability kernel
  and $f : \Cal{Y} \to \ol{\ul{\R}}$ be integrabel.
  Then $x \mapsto \int f \difd \kappa(\cdot \mid x)$ is measurable
  into $(\ol{\ul{\R}}, \ol{\ul{\bb{B}}})$.
  \label{prop:intKerMeas}
\end{prop}
\begin{proof}
  Simple functions are measurable since $\kappa$ is a kernel.
  Now extend by sums and limits.
\end{proof}

\subsubsection{Kernel derived processes}

Let $(\Cal{X}_n, \Sigma_{\Cal{X}_n})_{n \in \N}$ be a sequence
of measurable spaces. For each $n \in \N$ define
$\Cal{X}^{\ul{n}} \defeq \Cal{X}_1 \times \dots \times \Cal{X}_n$,
$\Sigma_{\Cal{X}^{\ul{n}}} \defeq \Sigma_{\Cal{X}_1} \otimes
\dots \otimes \Sigma_{\Cal{X}_n}$
and let
$\kappa_n : \Cal{X}^{\ul{n}} \leadsto \Cal{X}_{n+1}$ be a probability kernel.
Then $\kappa^{\ul{n}} \defeq \kappa_n \dots \kappa_1$ is a kernel
from $\Cal{X}_1$ to $\Cal{X}^{\ul{n}}$.
So for any probability measure $\rho_1 \in \Cal{P}(\Cal{X}_1)$
there exists a unique probability measure 
$\rho_n$ on $\Cal{X}^{\ul{n}}$ defined by
$\kappa^{\ul{n}} \rho_1$.

Let $\Cal{X}^{\ul{\infty}} \defeq \prod_{n \in \N} \Cal{X}_n$
and $\Sigma_{\Cal{X}^{\ul{\infty}}} \defeq \bigotimes_{n \in \N}
\Sigma_{\Cal{X}_n}$.
We are not equipped to establish existence of a
kernel generated measure on
$(\Cal{X}^{\ul{\infty}}, \Sigma_{\Cal{X}^{\ul{\infty}}})$ 
yet which we will need.
This problem was solved by Cassius Ionescu-Tulcea in 1949:

\begin{thm}[Ionescu-Tulcea extension theorem]
  For every $\mu \in \Cal{P}(\Cal{X}_1)$ 
  there exists a unique probability measure
  $\rho \in \Cal{P}(\Cal{X}^{\ul{\infty}})$ such that
  \[ \rho_n(A) = \rho \left( A \times \prod_{k=n+1}^\infty \Cal{X}_k \right)
  , \qquad \forall A \in \Sigma_{\Cal{X}^{\ul{n}}}, n \in \N \]
  We denote this measure
  $\dots \kappa_2 \kappa_1 \mu = \prod_{i=1}^\infty \kappa_i \mu \defeq \rho$.
  \label{thm:ionescuTulcea}
\end{thm}
\begin{proof}
  Todo: what about this.
\end{proof}

\begin{prop}
  Let $\mu_x$ denote the Ionescu-Tulcea measure of a
  sequence of probability kernels
  $\kappa_i : \Cal{X}^{\ul{i}} \to \Cal{X}_{i+1}$
  with starting measure $\delta_x$ on $\Cal{X}_1$ for any $x \in \Cal{X}_1$.
  Then $\kappa(A \mid x) = \mu_x(A)$ defines a probability kernel
  $\kappa : \Cal{X}_1 \to \Cal{X}^{\ul{\infty}}$.
\end{prop}
\begin{proof}
  Since we already know that $\mu_x$ is a probability measure for any
  $x \in \Cal{X}_1$,
  we just have to show that $\kappa(A \mid x) = \mu_x(A)$ is measurable for all
  $A \in \bigotimes_i \Sigma_{\Cal{X}_i}$.
  \dots todo
\end{proof}

\begin{lem}
  The Ionescu-Tulcea measure satisfies
  $\prod_{i=1}^\infty \kappa_i = \prod_{i=2}^\infty \kappa_i \kappa_1 $.
  \label{lem:ionescu}
\end{lem}
\begin{proof}
  Let $x \in \Cal{X}_1$.
  Notice that by associativity of the finitely induced measures
  $\kappa_n \dots \kappa_1 \delta_x
  = (\kappa_n \dots \kappa_2) (\kappa_1 \delta_x)$.
  This implies that
  \[ \prod_{i=1}^\infty \kappa_i \delta_x
    \left( A \times \prod_{k=n+1}^\infty \Cal{X}_k \right)
    = \prod_{i=2}^\infty \kappa_i \kappa_1 \delta_x
  \left( A \times \prod_{k=n+1}^\infty \Cal{X}_k \right) \]
  for all $n \in \N$ and $A \in \Sigma_{\Cal{X}^{\ul{n}}}$.
  By the uniqueness in \cref{thm:ionescuTulcea} we are done.
\end{proof}

\subsection{Dynamic programming}
In the quest to have a united framework to talk about results from
several different models we define here a quite general model.
One which is quite close to in generality can be found in
[ref. to Schal].
In this section recall that $\ul{\R} = \R \cup \{-\infty\}$,
$\ol{\R} = \R \cup \{\infty\}$ and
$\ol{\ul{\R}} = \R \cup \{\pm \infty\}$.

\begin{defn}[DP model]
  A general \defemph{dynamic programming} model is determined by
  \begin{enumerate}
    \item $(\Cal{S}_n, \Sigma_{\Cal{S}_n})_{n \in \N}$ a 
      measurable space of \defemph{states} for each timestep.
    \item $(\Cal{A}_n, \Sigma_{\Cal{A}_n})_{n \in \N}$ a 
      measurable space of \defemph{actions} for each timestep.
  \end{enumerate}
  for each $n \in \N$ we define the so called \defemph{history} spaces
  \[ \Cal{H}_n = \Cal{S}_1 \times \Cal{A}_1
    \times \Cal{S}_2 \times \ol{\ul{\R}} \times \Cal{A}_2
    \times \Cal{S}_3 \times \ol{\ul{\R}} \dots \times \Cal{S}_n,
    \Cal{H}_\infty = \Cal{S}_1 \times \Cal{A}_1 \times \Cal{S}_2 \times
    \ol{\ul{\R}} \times \dots
  \]
  with associated product $\sigma$-algebras
  \begin{enumerate} \setcounter{enumi}{2}
    \item $(P_n)_{n \in \N}$ a sequence of
      $\Cal{H}_n \times \Cal{A}_n \leadsto \Cal{S}_{n+1}$ kernels
      called the \defemph{transition} kernels.
    \item $(R_n)_{n \in \N}$ a sequence of
      $\Cal{H}_{n+1} \leadsto \ol{\ul{\R}}$ kernels
      called the \defemph{reward} kernels.
  \end{enumerate}
\end{defn}
Notice the slight irregularity in the beginning of the history spaces:
We are missing a reward state after $\Cal{S}_1$. We could avoid
this by introducing some start reward, but we will be careless.

The vast majority of sources considered in this paper actually specialize
the DP model with the following:
\begin{asm}[One state and action space]
  $\Cal{S}_1 = \Cal{S}_2 = \dots \defeq \Cal{S}$
  $\Cal{A}_1 = \Cal{A}_2 = \dots \defeq \Cal{A}$
  \label{asm:oneStateActionSpace}
\end{asm}
However we will do without this for the rest of this section in order to
present some results in the generality they deserve.
One could ask if it is possible to embed the general DP model into one
with \cref{asm:oneStateActionSpace} by setting
$\Cal{S} \defeq \Cal{S}^{\ul{\infty}}$ and
$\Cal{A} \defeq \Cal{A}^{\ul{\infty}}$.
I was not able to find discussions about this in litterature,
and chose not to pursue this end. %todo pursue this

For a DP model we can define
\begin{defn}[Policy]
  A (randomized) \defemph{policy} $\pi = (\pi_n)_{n \in \N}$
  is a sequence of $\Cal{H}_n \leadsto \Cal{A}_n$ kernels.
  The set of all policies we denote $R\Pi$.
  The policy $\pi$ is called \defemph{semi Markov} if each $\pi_i$ only depends
  on the first and last state in the history
  and is called \defemph{Markov} if only the last.
  The sets are denoted $sM\Pi$ and $M\Pi$.
  Furthermore $\pi$ is called \defemph{deterministic} if all $\pi_i$
  are degenerate, i.e. are actually measurable functions from
  $\Cal{H}_n$ to $\Cal{A}_n$. 
  Under \cref{asm:oneStateActionSpace}
  it makes sense to make a (Markov) policy $(\pi, \pi, \dots)$,
  such a policy is called \defemph{stationary},
  and the set of them denoted $S\Pi$.
\end{defn}
We have the following inclusions
\begin{align*}
  S\Pi \subseteq M\Pi &\subseteq sM\Pi \subseteq R\Pi
  \\ DS\Pi \subseteq DM\Pi &\subseteq DsM\Pi \subseteq D\Pi
\end{align*}

\begin{prop}
A dynamic progamming model together with a policy $\pi$ defines a
probability kernel $\kappa_\pi : \Cal{S}_1 \to \Cal{H}_\infty$.
\end{prop}
\begin{proof}
  This is the Ionescu-Tulcea kernel generated by
  $\dots R_2 P_2 \pi_2 R_1 P_1 \pi_1$.
\end{proof}
This kernel yields a probability measure $\kappa_\pi \mu$ on $\Cal{H}_\infty$
for every $\mu \in \Cal{S}_1$. In particular for any $s \in \Cal{S}_1$
$\kappa_\pi \delta_s$ yields the measure $\kappa(\cdot \mid s)$
and we shall occasionally write this $\kappa_\pi s$ and
integration with respect to it $\E^\pi_s$.

In litterature the terminology varies and generally %todo: backup this claim
any function mapping a state space $\Cal{S}$ to $\ol{\ul{\R}}$
can be called a (state)
\defemph{value} function. Similarly any $\ul{\ol{\R}}$ valued function
on pairs of states and actions can be called (state)
\defemph{action value} or \defemph{Q}- function.
The idea behind such functions are (usually) to estimate the
cumulative rewards associated with a state or state-action pair
and the trajectory of states it can lead to.
In order to define some of the most standard of value functions,
which we call \defemph{ideal} to avoid confusion, we will need one of
the following conditions:

\begin{cond}{$F^+$}
  $R_i(\{\infty\} \mid h) = 0$ for all
  $h \in \Cal{H}_{i+1}$ and $i \in \N$
  \label{cond:F+}
\end{cond}
\begin{cond}{$F^-$}
  $R_i(\{-\infty\} \mid h) = 0$ for all
  $h \in \Cal{H}_{i+1}$ and $i \in \N$
  \label{cond:F-}
\end{cond}
When assuming either of (\cref{cond:F+}) or (\cref{cond:F-})
adding rewards cannot lead to a $\infty - \infty$ situation,
and the following definition makes sense 
\begin{defn}[Ideal value functions]
  Let $r_i : \Cal{H}_\infty \to \ol{\ul{\R}}$ be the projection onto the
  $i$th reward. Define
  \[ V_{n,\pi}(s) = \E_s^\pi \sum_{i=1}^n r_i, \qquad
  V_\pi(s) = \E_s^\pi \limsup_{n \to \infty} \sum_{i=1}^n r_i \]
  called the \defemph{ideal} value functions.
\end{defn}

\begin{prop}
  The ideal value functions $V_{n,\pi}, V_\pi$ are measurable
  into $(\ul{\ol{\R}}, \ul{\ol{\bb{B}}})$.
\end{prop}
\begin{proof}  
  Use \cref{prop:intKerMeas}.
\end{proof}

Note that for pointwise convergence of $V_{n, \pi}$ to $V_\pi$
we need something like the assumptions for monotone or dominated convergence.
To this end we introduce 
\begin{cond}{P} $R_i \in [0,\infty], \forall i \in \N$
  \label{cond:P}
\end{cond}
\begin{cond}{N} $R_i \in [-\infty, 0], \forall i \in \N$ 
  \label{cond:N} 
\end{cond}
\begin{cond}{D} There exist a bound $R_{\max} > 0$ and a
  $\gamma \in [0,1)$ called the \defemph{discount} factor such that
  $R_i \in [-R_{\max} \gamma^i, R_{\max} \gamma^i]$
  for all $i \in \N$.
  \label{cond:D}
\end{cond}

\begin{prop}
  Under \cref{cond:P}, \cref{cond:N} or \cref{cond:D} we have
  $\lim_{n\to\infty} V_{n, \pi} = V_\pi $
  for all $\pi \in R\Pi$.
\end{prop}
\begin{proof}
  By monotone or dominated convergence.
\end{proof}

\subsubsection{Optimal policies}

Let $(\Cal{S}_n, \Cal{A}_n, P_n, R_n)_{n \in \N}$ be a DP model.

\begin{asm}(Reward independence)
  $P_n, R_n$ and policies are only allowed to depend on the
  states and actions.
  \label{asm:rewardIndep}
\end{asm}

In all sources known to this writer \cref{asm:rewardIndep} is assumed.
This is a bit of a puzzle since it is obvious that one could
want to define algorithms (policies) that take into account which rewards
they received in the past.
We will also do this but stick to the standard and 
never attempt to evaluate ideal value functions of
policies that depend on rewards.
Thus we will assume \cref{asm:rewardIndep} henceforth with including the
shrinkage of the set of general policies $R\Pi$ that it entails.

A neat consequence of \cref{asm:rewardIndep} when talking about
value functions is that we can reduce the reward kernels to functions
$r_i : \Cal{H}_{i+1} \to \ul{\R} = h \to \int r \difd R_i(r \mid h)$
which are measurable (due to \cref{prop:intKerMeas}).

\begin{defn}[Optimal value functions] 
  \begin{align*}
    V_n^*(s) \defeq & \; \sup_{\pi \in R\Pi} V_n^\pi(s) &
    V^*(s) \defeq & \; \sup_{\pi \in R\Pi} V^\pi(s)
  \end{align*}
  are called the \defemph{optimal} value functions.
  A policy $\pi^* \in R\Pi$ for which $V_{\pi^*} = V^*$ is called an
  \defemph{optimal} policy.
\end{defn}

An interesting fact about the optimal value functions is that they
might not be Borel measurable [todo ref to counterexample]
even in the finite case.
After all we are taking a supremum over
sets of policies which have cardinality of at least the continuum.
However it is sometimes possible to show that they are
universally measurable, thus Lebesgue measurable and therefore
standard Lebesgue integration is possible.
We will take these discussions as they occur in various settings.

At this point many interesting questions can be asked.
\begin{enumerate}
  \item To which extend does an optimal policy $\pi^*$ exist?
  \item Does $V_n^*$ converge to $V^*$?
  \item In case there is some sort of optimal policy
    in which classes of policies has a representative?
\end{enumerate}
These questions has been answered in a variety of settings.
We will address these question in order by strength of assumptions
they require as far as this is possible.

In a quite general setting, questions 1 and 2
was investigated by M. Schäl in 1974
[todo ref. to On Dynamic Programming:
Compactness of the space of policies, 1974].
Here some additional structure on our model is imposed:
\begin{sett}[Schäl]
  \begin{enumerate}
    \item $V_\pi < \infty$ for all policies $\pi \in R\Pi$.
    \item $(\Cal{S}_n, \Sigma_{\Cal{S}_n})$ is assumed to be standard Borel.
      I.e. $\Cal{S}_n$ is a non-empty Borel subset of a Polish space
      and $\Sigma_{\Cal{S}_n}$ is the Borel subsets of $S_n$.
    \item $(\Cal{A}_n, \Sigma_{\Cal{A}_n})$ is similarly assumed to be
      standard Borel.
    \item $\Cal{A}_n$ is compact.
    \item $\forall s \in \Cal{S}_1 :
      Z_n = \sup_{N \geq n} \sup_{\pi \in R\Pi} \sum_{t=n+1}^N
      \E_s^\pi r_n \to 0$ as $n \to \infty$.
  \end{enumerate}
  \label{sett:Schal}
\end{sett}

In this setting Schäl introduced two set of criteria for the existence
of an optimal policy:

\begin{cond}{S}
  \begin{enumerate}
    \item The function \[
	(a_1, a_2, \dots, a_n) \mapsto
	P_n(\cdot \mid s_1, a_1, s_2, a_2, \dots, s_n, a_n)
      \]
      is set-wise continuous (hence the name \defemph{S})
      for all $s_1, \dots, s_n \in \Cal{S}^{\ul{n}}$.
    \item $r_n$ is upper semi-continuous.
  \end{enumerate}
  \label{cond:S}
\end{cond}

\begin{cond}{W}
  \begin{enumerate}
    \item The function
      \[(h_n, a_n) \mapsto P_n(\cdot \mid h_n, a_n)\]
	is weakly continuous (hence the name \defemph{W}).
    \item $r_n$ is continuous.
  \end{enumerate}
  \label{cond:W}
\end{cond}

\begin{thm}[Existence and convergence of optimal policies in DP]
  When either \cref{cond:S} or \cref{cond:W} hold then
  \begin{enumerate}
    \item There exist an optimal policy $\pi^* \in R\Pi$.
    \item $V^*_n \to V^*$ as $n \to \infty$.
  \end{enumerate}
\end{thm}
\begin{proof}
  We refer to [todo ref: On Dynamic Programming: Compactness of the space of
  policies, M. Schäl 1974]. %todo: or do we?
\end{proof}

\subsection{Bertsekas-Shreve framework}
The theory here described is largely based on
[ref to Bertsekas-Shreve, Stochastic Optimal Control].
Their framework is cost-based as opposed to the this paper reward-based outset.
This means that positive and negative, upper and lower, supremum and infimum,
ect. are mirrored.
\begin{sett}[BS]
  We write the source notation in parenthesis for comparison.
  \begin{itemize}
    \item \Cref{asm:oneStateActionSpace} i.e. there is only one state
      and action space $\Cal{S}$, $\Cal{A}$.
    \item $P_n$ depends only on $s_n$ and $a_n$ and does not
      differ with $n$. I.e. there exists a kernel $P$ such that
      $P_n(\cdot \mid s_1, \dots, s_n, a_n) = P(\cdot \mid s_n, a_n)$
      for all $n \in \N$. We will write $P$ instead of $P_n$ understanding
      kernel compositions as if using $P_n$.
    \item $r_n$ depends only on $s_n$ and $a_n$ and does not differ
      with $n$ except for a potential discount.
      I.e. there exists a function $r:\Cal{S}\times\Cal{A}$ such that
      $r = r_n/\gamma^{n-1}$ for all $n \in \N$ (in the case where we 
      are not discounting set $\gamma = 1$).
    \item $\Cal{S}$ and $\Cal{A}$ are Borel spaces.
    \item $\Cal{A}$ is compact.
    \item $r$ is upper semicontinuous and bounded from above
      (least upper bound denoted $R_{\max} > 0$).
    \item $P(S \mid \cdot)$ is continuous for any $S \in \Sigma_{\Cal{S}}$.
  \end{itemize}
  \label{sett:BS}
\end{sett}
The original setup in [ref to Bertsekas-Shreve, Stochastic Optimal Control]
is slightly different than the setup here presented.
Besides having a state and action space, it also features a 
non-empty Borel space called the
\emph{disturbance space} $W$, a \emph{disturbance kernel}
$p: \Cal{S} \times \Cal{A} \to W$,
instead of a transition kernel which on the other hand is a deterministic
\emph{system function} $f : \Cal{S} \times \Cal{A} \times W \to \Cal{S}$
which should be Borel measurable.
Moreover it allows for constrains on the action space for each state.
This is made precise by a function $U:\Cal{S} \to \Sigma_{\Cal{A}}$
and a restriction on $R\Pi$ that all policies $\pi$ should satisfy
$\pi(U(s) \mid s) = 1$.
Lastly the rewards are interpreted as negative costs, and thus
$g$ is required to be semi \emph{lower}continuous.
This is equivalent to our conditions by symmetry.%todo prove this

By setting $P(\cdot \mid s, a) = f(s, a, p(\cdot \mid s, a))$
and maximizing rewards of upper semicontinuous instead of
minimizing lower semicontinuous ones, we fully capture
all aspects of the original model and its results,
except the for the action constrains.

With \cref{sett:BS} we can define

\begin{defn}[The $T$-operators]
  For a stationary policy $\pi$ we define
  the operators 
  \[ P^\pi V \defeq s \mapsto \int V(s') \difd P\pi(s' \mid s) \]
  \[ T^\pi V \defeq s \mapsto \int r(s, a)
  + \gamma V(s') \difd (P \pi)(s, a, s'\mid s) \]
  \[ T V \defeq s \mapsto \sup_{a \in \Cal{A}} T^a V(s) \]
  where $T^a = T^{\delta_a}$.
\end{defn}

%proposition 8.6 Stoch. Opt. Control
\begin{prop}
  $V^*_k = T^k 0$ and is semi uppercontinuous.
  Furthermore there exists a deterministic, Markov, Borel-measurable policy
  $\pi^* = (\pi^*_1, \pi^*_2, \dots) \in DM\Pi$
  which is $k$-optimal for all $k \in \N$.
\end{prop}

%cor. 9.17.2
\begin{thm} 
  Under (\cref{cond:N}) or (\cref{cond:D})
  $V^* = \lim_{k\to\infty} T^k 0$ and is upper semicontinuous.
  Furthermore there exist a deterministic
  stationary, Borel-measurable policy $\pi^*$.
\end{thm}

\begin{sett}[BS Analytic]
  The same as \cref{sett:BS} except:
  $P$ is not necessarily continuous.
  $r$ is upper semianalytic.
  $\Cal{A}$ is not necessarily compact.
  \label{sett:BSA}
\end{sett}

\begin{thm}
  Under \cref{sett:BSA}
  suppose there exists a $k \in \N$ such that
  $\forall \lambda \in \R, N \geq k, s \in \Cal{S}$
  \[ A^\lambda_N(s) = \left\{ a \in \Cal{A} \Mid r(s, a)
  + \gamma \int V^*_N P(\cdot \mid s, a) \geq \lambda \right\} \]
  is a compact subset of $\Cal{A}$.
  Then $V^*(s) = \lim_{N \to \infty} V^*_N(s)$ for all $s \in \Cal{S}$
  and there exists a optimal policy $\pi^*$ which is stationary
  and deterministic.
\end{thm}
\begin{proof}
  We refer to [todo ref to Bertsekas and Schreve, Stochastic Optimal Control:
  The Discrete-Time Case, prop. 9.17].
\end{proof}

\begin{prop}
  Under (\cref{cond:D}) for any $\pi \in R\Pi$ we have
  $V_{n,\pi}, V_\pi \leq V_{\max} \defeq R_{\max}/(1-\gamma)$.
\end{prop}
\begin{proof}
  \[ \sum_{i \in \N} \E_\mu r_i^+
    \leq \sum_{i \in \N} \gamma^{i-1} R_{\max}
  \leq R_{\max} / (1-\gamma) := V_{\max} < \infty \]
\end{proof}

\begin{prop}
  Under (\cref{cond:D})
  $T^\pi$ is $\gamma$-contractive on $\Cal{L}_\infty(\Cal{S})$.
  \label{prop:TpiContracts}
\end{prop}
\begin{proof}
  Let $V, V' \in \Cal{L}_\infty(\Cal{S})$
  and let $K = \norm{V - V'}_\infty$.
  Then
  \[ \norm{T^\pi V - T^\pi V'}_\infty
    = \sup_{s \in \Cal{S}} \abs{ \gamma
    \int V(s') - V'(s') \difd P\pi(s' \mid s)}
  \leq \gamma K \]
\end{proof}

\begin{cor}
  Under (\cref{cond:D})
  $V^\pi$ is the unique bounded fixed point of $T^\pi$.
\end{cor}
\begin{proof}
  This is by the Banach fixed point theorem and \cref{prop:TpiContracts}.
\end{proof}



\end{document}
