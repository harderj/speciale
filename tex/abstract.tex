
In this paper we present
\begin{enumerate}
  \item a framework for studying Q-learning for decision processes with
    in the generality of non-Markov dynamics and
    continuous state and action spaces
  \item sufficient criteria for existence of optimal policies in general
    (possibly non-Markov and with continuous state and action spaces)
    decision processes based on \mcite{S75} and \mcite{BSA83}
  \item relations between value-iteration and Q-iteration and their convergence
    properties in the setting of Markov decision processes with
    continuous state and action space
  \item bounds on deviations from optimality of Q-iteration when using function
    approximators focusing in particular on two function classes:
    \begin{enumerate}
      \item Artifical neural networks
      \item Bernstein polynomials
    \end{enumerate}
\end{enumerate}
